[
{
	"uri": "/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Xây dựng các ứng dụng RAG hiệu quả về chi phí với Knowledge Bases của Amazon Bedrock và Amazon S3 Vectors bởi Vaibhav Sabharwal, Ashish Lal, Dani Mitchell, và Irene Marban\nvào ngày 17 THÁNG 7 NĂM 2025\ntrong Amazon Bedrock, Amazon Bedrock Knowledge Bases, Amazon Machine Learning, Trí tuệ nhân tạo\nNhúng véc-tơ (vector embeddings) đã trở nên thiết yếu cho các ứng dụng Sinh dữ liệu tăng cường truy xuất (RAG) hiện đại, nhưng các tổ chức phải đối mặt với những thách thức đáng kể về chi phí khi mở rộng quy mô. Khi các cơ sở tri thức phát triển và yêu cầu các nhúng chi tiết hơn, nhiều cơ sở dữ liệu véc-tơ dựa trên lưu trữ hiệu suất cao như SSD hoặc giải pháp trong bộ nhớ trở nên đắt đỏ đến mức không thể chi trả. Rào cản chi phí này thường buộc các tổ chức phải giới hạn phạm vi của các ứng dụng RAG của họ hoặc thỏa hiệp về mức độ chi tiết của các biểu diễn véc-tơ, có khả năng ảnh hưởng đến chất lượng kết quả. Ngoài ra, đối với các trường hợp sử dụng liên quan đến dữ liệu lịch sử hoặc lưu trữ vẫn cần có thể tìm kiếm được, việc lưu trữ véc-tơ trong các cơ sở dữ liệu véc-tơ chuyên dụng được tối ưu hóa cho khối lượng công việc thông lượng cao là một khoản chi phí liên tục không cần thiết.\nBắt đầu từ ngày 15 tháng 7, khách hàng của Amazon Bedrock Knowledge Bases có thể chọn Amazon S3 Vectors (bản xem trước), dịch vụ lưu trữ đối tượng đám mây đầu tiên có hỗ trợ tích hợp để lưu trữ và truy vấn véc-tơ với chi phí thấp, làm kho lưu trữ véc-tơ. Người dùng Amazon Bedrock Knowledge Bases giờ đây có thể giảm chi phí tải lên, lưu trữ và truy vấn véc-tơ lên đến 90%. Được thiết kế để lưu trữ bền vững và tối ưu hóa chi phí cho các bộ dữ liệu véc-tơ lớn với hiệu suất truy vấn dưới một giây, S3 Vectors là lựa chọn lý tưởng cho các ứng dụng RAG yêu cầu lưu trữ dài hạn khối lượng véc-tơ khổng lồ và có thể chấp nhận sự đánh đổi về hiệu suất so với các cơ sở dữ liệu véc-tơ có số truy vấn mỗi giây (QPS) cao và độ trễ mili giây. Việc tích hợp với Amazon Bedrock có nghĩa là bạn có thể xây dựng các ứng dụng RAG tiết kiệm hơn trong khi vẫn duy trì hiệu suất tìm kiếm ngữ nghĩa cần thiết cho kết quả chất lượng.\nTrong bài đăng này, chúng tôi trình bày cách tích hợp Amazon S3 Vectors với Amazon Bedrock Knowledge Bases cho các ứng dụng RAG. Bạn sẽ học được một phương pháp thực tế để mở rộng quy mô cơ sở tri thức của mình để xử lý hàng triệu tài liệu trong khi vẫn duy trì chất lượng truy xuất và sử dụng bộ lưu trữ hiệu quả về chi phí của S3 Vectors.\nTổng quan về tích hợp Amazon Bedrock Knowledge Bases và Amazon S3 Vectors Khi tạo một knowledge base trong Amazon Bedrock, bạn có thể chọn S3 Vectors làm tùy chọn lưu trữ véc-tơ của mình. Sử dụng phương pháp này, bạn có thể xây dựng các ứng dụng RAG có thể mở rộng, hiệu quả về chi phí mà không cần cấp phép hoặc quản lý cơ sở hạ tầng phức tạp. Việc tích hợp mang lại sự tiết kiệm chi phí đáng kể trong khi vẫn duy trì hiệu suất truy vấn dưới một giây, làm cho nó trở nên lý tưởng để làm việc với các bộ dữ liệu véc-tơ lớn hơn được tạo ra từ khối lượng lớn dữ liệu phi cấu trúc bao gồm văn bản, hình ảnh, âm thanh và video. Sử dụng mô hình định giá trả theo mức sử dụng với các mức giá thấp, S3 Vectors cung cấp khả năng tối ưu hóa chi phí hàng đầu trong ngành, giúp giảm chi phí tải lên, lưu trữ và truy vấn véc-tơ lên đến 90% so với các giải pháp thay thế. Các khả năng tìm kiếm nâng cao bao gồm lọc siêu dữ liệu phong phú, vì vậy bạn có thể tinh chỉnh các truy vấn theo thuộc tính tài liệu như ngày tháng, danh mục và nguồn. Sự kết hợp giữa S3 Vectors và Amazon Bedrock là lý tưởng cho các tổ chức xây dựng các cơ sở tri thức quy mô lớn đòi hỏi cả hiệu quả chi phí và khả năng truy xuất hiệu quả—từ việc quản lý các kho tài liệu mở rộng đến các kho lưu trữ lịch sử và các ứng dụng yêu cầu biểu diễn véc-tơ chi tiết. Hướng dẫn này bao gồm các bước cấp cao sau:\nTạo một knowledge base mới Cấu hình nguồn dữ liệu Cấu hình nguồn dữ liệu và xử lý Đồng bộ hóa nguồn dữ liệu Kiểm tra knowledge base Điều kiện tiên quyết Trước khi bắt đầu, hãy đảm bảo rằng bạn có các điều kiện tiên quyết sau:\nMột tài khoản AWS có quyền truy cập dịch vụ phù hợp. Một vai trò AWS Identity and Access Management (IAM) với các quyền phù hợp để truy cập Amazon Bedrock và Amazon Simple Storage Service (Amazon S3). Bật quyền truy cập mô hình cho các mô hình nhúng và suy luận như Amazon Titan Text Embeddings V2 và Amazon Nova Pro. Hướng dẫn tích hợp Amazon Bedrock Knowledge Bases và Amazon S3 Vectors Trong phần này, chúng tôi sẽ hướng dẫn quy trình từng bước để tạo một knowledge base với Amazon S3 Vectors bằng cách sử dụng Bảng điều khiển quản lý AWS. Chúng tôi sẽ bao quát toàn bộ quy trình từ việc cấu hình kho lưu trữ véc-tơ đến việc nhập tài liệu và kiểm tra khả năng truy xuất của bạn.\nĐối với những người thích cấu hình knowledge base của họ theo chương trình thay vì sử dụng bảng điều khiển, kho lưu trữ Amazon Bedrock Knowledge Bases with S3 Vectors trên GitHub cung cấp một sổ tay hướng dẫn mà bạn có thể làm theo để triển khai thiết lập trong tài khoản của riêng mình.\nTạo một knowledge base mới Để tạo một knowledge base mới, hãy làm theo các bước sau:\nTrên bảng điều khiển Amazon Bedrock trong ngăn điều hướng bên trái, chọn Knowledge Bases. Để bắt đầu quá trình tạo, trong danh sách thả xuống Create, chọn Knowledge Base with vector store. Trên trang Provide Knowledge Base details, nhập tên mô tả cho knowledge base của bạn và một mô tả tùy chọn để xác định mục đích của nó. Chọn phương pháp cấp quyền IAM của bạn—tạo một vai trò dịch vụ mới hoặc sử dụng một vai trò hiện có—để cấp các quyền cần thiết để truy cập các dịch vụ AWS, như được hiển thị trong ảnh chụp màn hình sau. Chọn Amazon S3. Tùy chọn, thêm các thẻ để giúp tổ chức và phân loại tài nguyên của bạn và cấu hình các đích đến để gửi nhật ký như một bucket S3 hoặc Amazon CloudWatch để theo dõi và khắc phục sự cố. Chọn Next để tiếp tục đến phần cấu hình nguồn dữ liệu. Cấu hình nguồn dữ liệu Để cấu hình nguồn dữ liệu, hãy làm theo các bước sau:\nGán một tên mô tả cho dữ liệu knowledge base của bạn. Trong Data source location, chọn xem bucket S3 tồn tại trong tài khoản AWS hiện tại của bạn hay một tài khoản khác, sau đó chỉ định vị trí lưu trữ tài liệu của bạn, như được hiển thị trong ảnh chụp màn hình sau. Trong bước này, hãy cấu hình chiến lược phân tích cú pháp của bạn để xác định cách Amazon Bedrock xử lý tài liệu của bạn. Chọn Amazon Bedrock default parser cho các tài liệu chỉ có văn bản mà không mất thêm chi phí. Chọn Amazon Bedrock Data Automation as parser or Foundation models as a parser để xử lý các tài liệu phức tạp có các yếu tố trực quan.\nCấu hình chiến lược phân đoạn cũng quan trọng không kém vì nó xác định cách nội dung của bạn được phân đoạn thành các đơn vị có ý nghĩa để nhúng véc-tơ, ảnh hưởng trực tiếp đến chất lượng truy xuất và bảo toàn ngữ cảnh. Chúng tôi đã chọn Fixed-size chunking cho ví dụ này do kích thước token có thể dự đoán và sự đơn giản của nó. Vì cả quyết định phân tích cú pháp và phân đoạn đều không thể sửa đổi sau khi tạo, hãy chọn các tùy chọn phù hợp nhất với cấu trúc nội dung và nhu cầu truy xuất của bạn. Đối với dữ liệu nhạy cảm, bạn có thể sử dụng cài đặt nâng cao để triển khai mã hóa AWS Key Management Service (AWS KMS) hoặc áp dụng các hàm chuyển đổi tùy chỉnh để tối ưu hóa tài liệu của bạn trước khi nhập. Theo mặc định, S3 Vectors sẽ sử dụng mã hóa phía máy chủ (SSE-S3).\nCấu hình lưu trữ và xử lý dữ liệu Để cấu hình lưu trữ và xử lý dữ liệu, trước tiên hãy chọn mô hình nhúng, như được hiển thị trong ảnh chụp màn hình sau. Mô hình nhúng sẽ chuyển đổi các đoạn văn bản của bạn thành các biểu diễn véc-tơ số cho khả năng tìm kiếm ngữ nghĩa. Nếu kết nối với một S3 Vector hiện có làm kho lưu trữ véc-tơ, hãy đảm bảo rằng kích thước của mô hình nhúng khớp với kích thước được sử dụng khi tạo kho lưu trữ véc-tơ của bạn vì sự không khớp về kích thước sẽ gây ra lỗi nhập. Amazon Bedrock cung cấp một số mô hình nhúng để lựa chọn, mỗi mô hình có các kích thước véc-tơ và đặc điểm hiệu suất khác nhau được tối ưu hóa cho các trường hợp sử dụng khác nhau. Hãy xem xét cả sự phong phú về ngữ nghĩa của mô hình và các tác động về chi phí của nó khi đưa ra lựa chọn của bạn.\nTiếp theo, cấu hình kho lưu trữ véc-tơ. Đối với việc lựa chọn lưu trữ véc-tơ, hãy chọn cách Amazon Bedrock Knowledge Bases sẽ lưu trữ và quản lý các nhúng véc-tơ được tạo từ tài liệu của bạn trong Amazon S3 Vectors, bằng một trong hai tùy chọn sau:\nTùy chọn 1. Nhanh chóng tạo một kho lưu trữ véc-tơ mới Tùy chọn được khuyến nghị này, được hiển thị trong ảnh chụp màn hình sau, tự động tạo một bucket S3 vector trong tài khoản của bạn trong quá trình tạo knowledge base. Hệ thống sẽ tối ưu hóa việc lưu trữ véc-tơ của bạn để lưu trữ các bộ dữ liệu véc-tơ quy mô lớn một cách bền vững và hiệu quả về chi phí, tạo ra một bucket S3 vector và một chỉ mục véc-tơ cho bạn.\nTùy chọn 2. Sử dụng một kho lưu trữ véc-tơ hiện có Khi tạo chỉ mục Amazon S3 Vector làm kho lưu trữ véc-tơ để sử dụng với Amazon Bedrock Knowledge Bases, bạn có thể đính kèm siêu dữ liệu (chẳng hạn như năm, tác giả, thể loại và vị trí) dưới dạng các cặp khóa-giá trị cho mỗi véc-tơ. Theo mặc định, các trường siêu dữ liệu có thể được sử dụng làm bộ lọc trong các truy vấn tương tự trừ khi được chỉ định là siêu dữ liệu không thể lọc tại thời điểm tạo chỉ mục véc-tơ. Các chỉ mục S3 Vector hỗ trợ các loại chuỗi, số và Boolean lên đến 40 KB mỗi véc-tơ, với siêu dữ liệu có thể lọc được giới hạn ở mức 2 KB mỗi véc-tơ.\nĐể chứa các đoạn văn bản lớn hơn và siêu dữ liệu phong phú hơn trong khi vẫn cho phép lọc trên các thuộc tính quan trọng khác, hãy thêm \u0026ldquo;AMAZON_BEDROCK_TEXT\u0026rdquo; vào danh sách nonFilterableMetadataKeys trong cấu hình chỉ mục của bạn. Phương pháp này tối ưu hóa việc phân bổ bộ nhớ của bạn cho nội dung tài liệu trong khi vẫn duy trì khả năng lọc cho các thuộc tính có ý nghĩa như danh mục hoặc ngày tháng. Hãy nhớ rằng các trường được thêm vào mảng nonFilterableMetadataKeys không thể được sử dụng với tính năng lọc siêu dữ liệu trong các truy vấn và không thể sửa đổi sau khi chỉ mục được tạo.\nĐây là một ví dụ về việc tạo một chỉ mục Amazon S3 Vector với cấu hình siêu dữ liệu phù hợp:\ns3vectors.create_index( vectorBucketName=\u0026#34;my-first-vector-bucket\u0026#34;, indexName=\u0026#34;my-first-vector-index\u0026#34;, dimension=1024, distanceMetric=\u0026#34;cosine\u0026#34;, dataType=\u0026#34;float32\u0026#34;, metadataConfiguration={\u0026#34;nonFilterableMetadataKeys\u0026#34;: [\u0026#34;AMAZON_BEDROCK_TEXT\u0026#34;]} ) Để biết chi tiết về cách tạo kho lưu trữ véc-tơ, hãy tham khảo Giới thiệu Amazon S3 Vectors trong Blog Tin tức của AWS.\nSau khi bạn có một bucket và chỉ mục S3 Vector, bạn có thể kết nối nó với knowledge base của mình. Bạn sẽ cần cung cấp cả Tên tài nguyên Amazon (ARN) của bucket S3 Vector và ARN của chỉ mục véc-tơ, như được hiển thị trong ảnh chụp màn hình sau, để liên kết chính xác knowledge base của bạn với chỉ mục S3 Vector hiện có.\nĐồng bộ hóa nguồn dữ liệu Sau khi bạn đã cấu hình knowledge base của mình với S3 Vectors, bạn cần đồng bộ hóa nguồn dữ liệu của mình để tạo và lưu trữ các nhúng véc-tơ. Từ bảng điều khiển Amazon Bedrock Knowledge Bases, hãy mở knowledge base đã tạo của bạn và tìm nguồn dữ liệu đã cấu hình của bạn và chọn Sync để bắt đầu quá trình, như được hiển thị trong ảnh chụp màn hình sau. Trong quá trình đồng bộ hóa, hệ thống sẽ xử lý tài liệu của bạn theo cấu hình phân tích cú pháp và phân đoạn của bạn, tạo các nhúng bằng mô hình bạn đã chọn và lưu trữ chúng trong chỉ mục S3 vector của bạn. Bạn có thể theo dõi tiến trình đồng bộ hóa trong thời gian thực nếu bạn đã cấu hình Nhật ký Amazon CloudWatch và xác minh trạng thái hoàn thành trước khi kiểm tra khả năng truy xuất của knowledge base.\nKiểm tra knowledge base Sau khi cấu hình thành công knowledge base của bạn với S3 Vectors, bạn có thể xác thực chức năng của nó bằng giao diện kiểm tra tích hợp. Bạn có thể sử dụng bảng điều khiển tương tác này để thử nghiệm với các loại truy vấn khác nhau và xem cả kết quả truy xuất và các phản hồi được tạo ra. Chọn giữa chế độ Chỉ truy xuất (API Retrieve) để kiểm tra các đoạn nguồn thô hoặc Truy xuất và tạo phản hồi (API RetrieveandGenerate) để tìm hiểu cách các mô hình nền tảng (FM) như Amazon Nova sử dụng nội dung được truy xuất của bạn. Giao diện kiểm tra cung cấp những hiểu biết có giá trị về cách knowledge base của bạn xử lý các truy vấn, hiển thị các đoạn nguồn, điểm liên quan của chúng và siêu dữ liệu liên quan.\nBạn cũng có thể cấu hình cài đặt truy vấn cho knowledge base của mình giống như với các tùy chọn lưu trữ véc-tơ khác, bao gồm các bộ lọc để lựa chọn dựa trên siêu dữ liệu, các rào cản để có phản hồi phù hợp, khả năng xếp hạng lại và các tùy chọn sửa đổi truy vấn. Những công cụ này giúp tối ưu hóa chất lượng truy xuất và đảm bảo thông tin phù hợp nhất được trình bày cho các FM của bạn. S3 Vectors hiện hỗ trợ chức năng tìm kiếm ngữ nghĩa. Sử dụng việc xác thực thực hành này, bạn có thể tinh chỉnh cấu hình của mình trước khi tích hợp knowledge base với các ứng dụng sản xuất.\nTạo knowledge base Amazon Bedrock của bạn theo chương trình Trong các phần trước, chúng tôi đã hướng dẫn cách tạo một knowledge base với Amazon S3 Vectors bằng Bảng điều khiển quản lý AWS. Đối với những người thích tự động hóa quy trình này hoặc tích hợp nó vào các quy trình làm việc hiện có, bạn cũng có thể tạo knowledge base của mình theo chương trình bằng cách sử dụng AWS SDK.\nSau đây là một đoạn mã mẫu cho thấy lệnh gọi API trông như thế nào khi tạo một knowledge base Amazon Bedrock theo chương trình với một chỉ mục Amazon S3 Vector hiện có:\nresponse = bedrock.create_knowledge_base( description=\u0026#39;Amazon Bedrock Knowledge Base integrated with Amazon S3 Vectors\u0026#39;, knowledgeBaseConfiguration={ \u0026#39;type\u0026#39;: \u0026#39;VECTOR\u0026#39;, \u0026#39;vectorKnowledgeBaseConfiguration\u0026#39;: { \u0026#39;embeddingModelArn\u0026#39;: f\u0026#39;arn:aws:bedrock:{region}::foundation-model/amazon.titan-embed-text-v2:0\u0026#39;, \u0026#39;embeddingModelConfiguration\u0026#39;: { \u0026#39;bedrockEmbeddingModelConfiguration\u0026#39;: { \u0026#39;dimensions\u0026#39;: vector\\_dimension, \\#Verify this is the same value as S3 vector index configuration \u0026#39;embeddingDataType\u0026#39;: \u0026#39;FLOAT32\u0026#39; } }, }, }, name=knowledge\\_base\\_name, roleArn=roleArn, storageConfiguration={ \u0026#39;s3VectorsConfiguration\u0026#39;: { \u0026#39;indexArn\u0026#39;: vector\\_index\\_arn }, \u0026#39;type\u0026#39;: \u0026#39;S3\\_VECTORS\u0026#39; } ) Vai trò được gắn với knowledge base nên có một số chính sách được đính kèm, bao gồm quyền truy cập vào API S3 Vectors, các mô hình được sử dụng để nhúng, tạo và xếp hạng lại (nếu được sử dụng), và bucket S3 được sử dụng làm nguồn dữ liệu. Nếu bạn đang sử dụng khóa do khách hàng quản lý cho S3 Vector làm kho lưu trữ véc-tơ, bạn sẽ cần cung cấp một chính sách bổ sung để cho phép giải mã dữ liệu. Sau đây là chính sách cần thiết để truy cập Amazon S3 Vector làm kho lưu trữ véc-tơ:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: \\[ { \u0026#34;Sid\u0026#34;: \u0026#34;BedrockInvokeModelPermission\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \\[ \u0026#34;bedrock:InvokeModel\u0026#34; \\], \u0026#34;Resource\u0026#34;: \\[ \u0026#34;arn:aws:bedrock:{REGION}::foundation-model/amazon.titan-embed-text-v2:0\u0026#34; \\] }, { \u0026#34;Sid\u0026#34;: \u0026#34;KmsPermission\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \\[ \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:Decrypt\u0026#34; \\], \u0026#34;Resource\u0026#34;: \\[ \u0026#34;arn:aws:kms:{REGION}:{ACCOUNT\\_ID}:key/{KMS\\_KEY\\_ID}\u0026#34; \\] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3ListBucketPermission\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \\[ \u0026#34;s3:ListBucket\u0026#34; \\], \u0026#34;Resource\u0026#34;: \\[ \u0026#34;arn:aws:s3:::{SOURCE\\_BUCKET\\_NAME}\u0026#34; \\], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ResourceAccount\u0026#34;: \\[ \u0026#34;{ACCOUNT\\_ID}\u0026#34; \\] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3GetObjectPermission\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \\[ \u0026#34;s3:GetObject\u0026#34; \\], \u0026#34;Resource\u0026#34;: \\[ \u0026#34;arn:aws:s3:::{SOURCE\\_BUCKET\\_NAME}/{PREFIX}/\\*\u0026#34; \\], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ResourceAccount\u0026#34;: \\[ \u0026#34;{ACCOUNT\\_ID}\u0026#34; \\] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3VectorsAccessPermission\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \\[ \u0026#34;s3vectors:GetIndex\u0026#34;, \u0026#34;s3vectors:QueryVectors\u0026#34;, \u0026#34;s3vectors:PutVectors\u0026#34;, \u0026#34;s3vectors:GetVectors\u0026#34;, \u0026#34;s3vectors:DeleteVectors\u0026#34; \\], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3vectors:{REGION}:{ACCOUNT\\_ID}:bucket/{VECTOR\\_BUCKET\\_NAME}/index/{VECTOR\\_INDEX\\_NAME}\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ResourceAccount\u0026#34;: \u0026#34;{ACCOUNT\\_ID}\u0026#34; } } } ] } Dọn dẹp Để dọn dẹp tài nguyên của bạn, hãy hoàn thành các bước sau. Để xóa knowledge base:\nTrên bảng điều khiển Amazon Bedrock, chọn Knowledge Bases Chọn Knowledge Base của bạn và ghi lại cả tên vai trò dịch vụ IAM và ARN chỉ mục S3 Vector Chọn Delete và xác nhận Để xóa S3 Vector làm kho lưu trữ véc-tơ, hãy sử dụng các lệnh sau của Giao diện dòng lệnh AWS (AWS CLI):\naws s3vectors delete-index --vector-bucket-name YOUR_VECTOR_BUCKET_NAME --index-name YOUR_INDEX_NAME --region YOUR_REGION aws s3vectors delete-vector-bucket --vector-bucket-name YOUR_VECTOR_BUCKET_NAME --region YOUR_REGION Trên bảng điều khiển IAM, tìm vai trò đã ghi chú trước đó Chọn và xóa vai trò Để xóa bộ dữ liệu mẫu:\nTrên bảng điều khiển Amazon S3, tìm bucket S3 của bạn Chọn và xóa các tệp bạn đã tải lên cho hướng dẫn này Kết luận Sự tích hợp giữa Amazon Bedrock Knowledge Bases và Amazon S3 Vectors đại diện cho một bước tiến đáng kể trong việc làm cho các ứng dụng RAG trở nên dễ tiếp cận và khả thi về mặt kinh tế hơn ở quy mô lớn. Bằng cách sử dụng bộ lưu trữ được tối ưu hóa chi phí của Amazon S3 Vectors, các tổ chức giờ đây có thể xây dựng các cơ sở tri thức ở quy mô lớn với hiệu quả chi phí được cải thiện. Điều này có nghĩa là khách hàng có thể đạt được sự cân bằng tối ưu giữa hiệu suất và kinh tế, và bạn có thể tập trung vào việc tạo ra giá trị thông qua các ứng dụng do AI cung cấp thay vì quản lý cơ sở hạ tầng lưu trữ véc-tơ phức tạp.\nĐể bắt đầu với việc tích hợp Amazon Bedrock Knowledge Bases và Amazon S3 Vectors, hãy tham khảo Sử dụng S3 Vectors với Amazon Bedrock Knowledge Bases trong Hướng dẫn sử dụng Amazon S3.\nVề các tác giả Vaibhav Sabharwal là một Kiến trúc sư Giải pháp Cấp cao của Amazon Web Services (AWS) có trụ sở tại New York. Anh đam mê học hỏi các công nghệ đám mây mới và hỗ trợ khách hàng xây dựng chiến lược áp dụng đám mây, thiết kế các giải pháp sáng tạo và thúc đẩy sự xuất sắc trong vận hành. Là một thành viên của Cộng đồng Kỹ thuật Dịch vụ Tài chính tại AWS, anh tích cực đóng góp vào các nỗ lực hợp tác trong ngành.\nDani Mitchell là một Kiến trúc sư Giải pháp Chuyên gia về AI Tạo sinh tại Amazon Web Services (AWS). Anh tập trung vào việc giúp các doanh nghiệp trên toàn thế giới đẩy nhanh hành trình AI tạo sinh của họ với Amazon Bedrock.\nIrene Marban là một Kiến trúc sư Giải pháp Chuyên gia về AI Tạo sinh tại Amazon Web Services (AWS), làm việc với các khách hàng trên khắp EMEA để thiết kế và triển khai các giải pháp AI tạo sinh nhằm thúc đẩy hoạt động kinh doanh của họ. Với nền tảng về kỹ thuật y sinh và AI, công việc của cô tập trung vào việc giúp các tổ chức tận dụng các công nghệ AI mới nhất để thúc đẩy sự đổi mới và tăng trưởng. Khi rảnh rỗi, cô thích đọc sách và nấu ăn cho bạn bè.\nAshish Lal là Giám đốc Tiếp thị Sản phẩm Cấp cao về AI/ML cho Amazon Bedrock. Anh có hơn 11 năm kinh nghiệm trong lĩnh vực tiếp thị sản phẩm và thích giúp khách hàng đẩy nhanh thời gian tạo ra giá trị và giảm chi phí vòng đời AI của họ.\n"
},
{
	"uri": "/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Đỗ Đăng Khoa\nPhone Number: 0783759971\nEmail: khoado7577@gmail.com\nUniversity: FPT University\nMajor: Software Engineering/ IC design major\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Set up development environment and understand FCJ program workflows. Create AWS Free Tier account and explore core AWS services. Establish documentation workflow for the internship journey. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Get familiarized with FCJ members - Understand FCJ program workflows and rules - Initial onboarding and orientation 09/08/2025 09/08/2025 2 - Install Linux Fedora KDE Plasma 42 - Learn basic Linux setup and configuration - Troubleshoot installation issues 09/09/2025 09/09/2025 3 - Create AWS Free Tier account - Explore AWS Free Tier services - Complete tasks to obtain $200 AWS credit 09/10/2025 09/10/2025 https://aws.amazon.com/free/ 4 - Continue AWS Free Tier exploration - Review available services: EC2, RDS, Lambda, VPC - Understand service limits and free tier constraints 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Explore S3 buckets and storage options - Learn AWS Budget management - Set up daily and monthly budget alerts - Continue Linux troubleshooting 09/12/2025 09/12/2025 https://aws.amazon.com/s3/ 6 - Finalize budget configurations - Review Billing \u0026amp; Cost Management dashboard - Complete Linux installation troubleshooting - Set up GitHub repos for documentation 09/13/2025 09/13/2025 Week 1 Achievements: Successfully integrated with the First Cloud Journey team and understood the program structure, workflows, and expectations.\nInstalled and configured Linux Fedora KDE Plasma 42 as the primary development environment:\nCompleted basic Linux setup Resolved installation and configuration issues Familiarized with KDE Plasma desktop environment Created and configured AWS Free Tier account:\nSuccessfully registered for AWS Free Tier Completed required tasks to obtain $200 in AWS credits Explored available free tier services Gained hands-on experience with core AWS services:\nEC2 - Elastic Compute Cloud for virtual servers RDS - Relational Database Service Lambda - Serverless compute service VPC - Virtual Private Cloud networking S3 - Simple Storage Service for object storage Implemented AWS cost management best practices:\nSet up AWS Budgets for cost monitoring Configured daily budget alerts Configured monthly budget thresholds Explored Billing \u0026amp; Cost Management dashboard Established documentation workflow:\nCreated GitHub repositories for AWS FCJ journey documentation Set up templates for efficient worklog management Organized project structure for ongoing documentation Acquired foundational knowledge:\nBasic AWS account management and navigation FCJ program rules and operational workflows Linux system administration basics Cost optimization strategies for cloud resources AWS Free Tier services overview\nDaily and monthly budget alerts configuration\nChallenges Faced: Minor Linux installation issues that required troubleshooting and configuration adjustments. Initial learning curve with AWS console navigation and service discovery. Key Learnings: Understanding the importance of cost management from day one in cloud environments. The value of proper documentation and workflow templates for long-term project success. Basic Linux system administration skills are essential for cloud computing work. Next Week Goals: Deep dive into specific AWS services based on project requirements. Continue building documentation and refining workflow processes. Explore more advanced AWS features and best practices. "
},
{
	"uri": "/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud Journey - 12 Week Program This worklog documents my 12-week journey through the AWS First Cloud Journey program, from September 6, 2025 to November 30, 2025. During this intensive program, I gained comprehensive knowledge of AWS cloud services, attended major AWS events, completed a team project, and built a strong foundation for a career in cloud computing.\nProgram Overview The program combined individual AWS learning, hands-on labs, team collaboration, and participation in AWS community events. Despite facing challenges including a hand injury in Week 10, I successfully completed all program requirements and demonstrated resilience and adaptability throughout the journey.\nWeekly Progress Week 1: Onboarding and AWS Foundation\nProgram kickoff, AWS FCJ event at Bitexco, AWS fundamentals, account setup\nWeek 2: Development Environment Setup and AWS Cloud Day 2025\nAWS Cloud Day 2025 (GenAI track), development tools, IAM basics\nWeek 3: VPC and EC2 Fundamentals with Static Website Deployment\nNetworking concepts, EC2 instances, static website deployment\nWeek 4: AWS Networking and Storage Deep Dive\nVPC architecture, S3 storage, RDS introduction\nWeek 5: Hands-on Labs and Blog Translation Start\nVPC/S3 labs, RDS hands-on, Lambda introduction, first blog translation\nWeek 6: Data Science on AWS Workshop\nData Science workshop at FPT University, AWS Glue, SageMaker, API Gateway\nWeek 7: Container Services and Blog Publication\nPublished first blog, ECS/ECR, security best practices, second blog started\nWeek 8: Midterm Exam Week\nAWS Cloud Practitioner exam preparation and completion\nWeek 9: Team Project Kickoff\nTeam formation, project planning, architecture design\nWeek 10: Project Development and AWS Cloud Mastery #1\nProject sprint, hand injury (14/11), AWS Cloud Mastery #1 on AI/ML/GenAI (15/11)\nWeek 11: AWS Cloud Mastery #2 and Recovery\nAWS Cloud Mastery #2 on DevOps (17/11), adapted workflow, doctor revisit (21/11)\nWeek 12: Project Completion and AWS Cloud Mastery #3\nFinal project delivery, AWS Cloud Mastery #3 on Security (29/11), program wrap-up\nMajor Achievements ✅ Completed 12-week AWS First Cloud Journey program ✅ Attended 6 major AWS events and workshops ✅ Passed AWS Cloud Practitioner midterm exam ✅ Translated 2+ AWS technical blogs ✅ Completed team project successfully ✅ Attended all 3 AWS Cloud Mastery Series workshops ✅ Overcame injury and adapted workflow ✅ Built comprehensive AWS knowledge foundation Events Attended AWS FCJ Kick-off (6/9/2025) - Bitexco Financial Tower AWS Cloud Day 2025 - GenAI and Data Track Data Science on AWS Workshop (16/10/2025) - FPT University AWS Cloud Mastery Series #1 - AI/ML/GenAI (15/11/2025) AWS Cloud Mastery Series #2 - DevOps (17/11/2025) AWS Cloud Mastery Series #3 - Security (29/11/2025) Skills Developed AWS Core Services: EC2, S3, VPC, RDS, Lambda Serverless Architecture and GenAI Applications Data Science and ML on AWS (SageMaker, Bedrock) DevOps Practices and CI/CD Pipelines Security Best Practices and Well-Architected Framework Team Collaboration and Project Management Technical Documentation and Presentation Resilience and Adaptability "
},
{
	"uri": "/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders - GenAI and Data Track Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Thursday, September 18, 2025\nLearning Report: \u0026ldquo;Vietnam Cloud Day 2025: GenAI and Data Track\u0026rdquo; Event Objectives Provide an overview of Agentic AI and AWS\u0026rsquo;s strategic vision Understand how to build a Unified Data Foundation to effectively support AI activities and Data Analytics on AWS Analyze in detail the GenAI deployment roadmap, AI Agent Architecture models, and challenges in moving them to production Study the AI-Driven Development Lifecycle (AI-DLC) model Master core principles of Security, Risk Management, and Responsible AI in Generative AI Introduce new AWS services designed to support AI Agents and maximize business productivity Speaker List Jun Kai Loke - Solution Architecture Expert for AI/ML, AWS Kien Nguyen - Solution Architect, AWS Tamelly Lim - Storage Solution Architecture Expert, AWS Binh Tran - Senior Solution Architect, AWS Taiki Dang - Solution Architect, AWS Christal Poon - Solution Architecture Expert, AWS Key Content Highlights Overview of Agentic AI – Jun Kai Loke Agentic AI is an important strategic trend focusing on creating systems with autonomous capabilities, minimizing human intervention, and automating complex processes at a deep level Real-world examples of successful applications: Katalon, Apero, Techcom Securities Amazon Bedrock serves as the core platform for AI development, supporting: Secure deployment at scale Integration of tools and memory capabilities End-to-end monitoring Building a Unified Data Foundation on AWS – Kien Nguyen Current Challenges: Many businesses struggle with GenAI deployment due to unprepared data platforms (only 52% of CDOs assess their data platforms as ready, according to HBR), mainly due to data silos, people silos, and separate business units End-to-End Data Strategy includes three interacting components: Producers, Foundations, and Consumers Essential AWS Data Services: Amazon Bedrock (GenAI platform) Databases (RDS and specialized services supporting vector search) Analytics \u0026amp; ML (SageMaker, Unified Studio) Data \u0026amp; AI Governance Lake House Architecture (S3, Redshift Managed Storage, Iceberg Open API) Amazon DataZone (Data management and sharing) GenAI Roadmap \u0026amp; AI Agent Architecture – Jun Kai Loke \u0026amp; Tamelly Lim AI Agents building blueprint includes: Model \u0026amp; application capabilities, and tool framework AWS introduces Amazon Bedrock AgentCore to address challenges in moving Agents to production AgentCore includes: Agent Core Runtime, Agent Core Gateway, Memory, Agent Browser and Code Interpreter, aiming to enhance security and scalability AI-Driven Development Lifecycle (AI-DLC) – Binh Tran AI-DLC is a new software development model, maximally automated by AI, consisting of 3 stages:\nInception: Define context, outline user stories, and plan with work units Construction: Code + test, supplement architecture, deploy Infrastructure as Code (IaC) and test Operation: Deploy to production using IaC and incident management Security of Generative AI Applications – Taiki Dang Essential Security Elements: Compliance \u0026amp; Governance, Legal \u0026amp; Privacy, Controls, Risk Management, and Resilience Risk Analysis by Layer: End-user risks (Hallucination, IP, Legal), fine-tuning risks (data retention), and model provider risks (training data, model construction) Risk Mitigation Strategies: Use Prompt engineering, Fine-tuning, Retrieval-Augmented Generation (RAG), parameter adjustment, Bedrock Guardrails, and prompt security Must apply standards such as AWS Well-Architected, MITRE ATLAS, OWASP Top 10 for LLM Apps, NIST AI 600-1, ISO 42001, and EU AI Act AI Agents: Boosting Business Productivity – Christal Poon Introduction to AI Agent types: Specialized Agents, Fully-managed Agents, and DIY Agents Productivity support services: Amazon QuickSight (for business analytics) and Amazon Q (providing Dashboards, Reports, Executive summaries, and AI Agent scenarios) What I Learned Mindset \u0026amp; Strategy Agentic AI is the next evolution of automation, moving towards autonomous systems with reduced human oversight Strong data platforms (based on S3, Lake House, Bedrock, SageMaker) are prerequisites for successful GenAI deployment AI-DLC provides a modern methodology that automates the entire development cycle from planning, coding, testing to deployment Architecture \u0026amp; Technology Understanding AI Agent architecture and Amazon Bedrock AgentCore solutions addresses challenges around security and scalability in production environments Security must be integrated at every level of the AI stack, from data, model to end-user application, while complying with international standards and regulations Technology Application AWS is significantly investing and expanding the AI Agents \u0026amp; Enterprise AI ecosystem through services like Amazon Q and QuickSuite (coming soon) Application to Work Process Optimization: Research and integrate AI Agents into repetitive business tasks to increase efficiency Quality Management: Use Amazon Bedrock, Amazon Q, and Guardrails to control quality, ensure safety, and minimize model \u0026ldquo;hallucination\u0026rdquo; Infrastructure Building: Ensure building a unified data platform with clear governance before launching GenAI projects Applying AI-DLC: Experiment with AI-DLC software development model in internal projects to accelerate deployment Business Analysis: Use QuickSight and Amazon Q to quickly create dashboards and insights for leadership and business teams Event Experience The workshop provided clear and practical insight into the transition from traditional automation to the Agentic AI era. Speakers\u0026rsquo; presentations provided specific guidance for the GenAI adoption journey in Vietnam. The combination of AgentCore, Bedrock, AI-DLC, and Amazon Q painted a comprehensive and powerful picture of the next generation of AI for enterprises, from data platforms, application development, to security and operations.\nAdd your event photos here Overall, the event not only provided in-depth technical knowledge but also helped shift thinking about how to strategically, responsibly, and safely integrate AI into all aspects of business.\nSpeakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Đánh giá các mô hình AI tạo sinh với Amazon Nova LLM-as-a-Judge trên Amazon SageMaker AI bởi Surya Kari, Joel Carlson, Michael Cai, Morteza Ziyadi, Pradeep Natarajan, và Saurabh Sahu | vào ngày 17 tháng 7 2025 | trong Amazon Nova, Amazon SageMaker, Amazon SageMaker AI, Thông báo, Trí tuệ nhân tạo, Các mô hình nền tảng | Permalink | Bình luận | Chia sẻ\nViệc đánh giá hiệu suất của các mô hình ngôn ngữ lớn (LLM) vượt ra ngoài các chỉ số thống kê như độ phức tạp (perplexity) hay điểm đánh giá song ngữ (BLEU). Đối với hầu hết các kịch bản AI tạo sinh trong thế giới thực, điều quan trọng là phải hiểu liệu một mô hình có đang tạo ra các kết quả đầu ra tốt hơn so với một mô hình cơ sở hoặc một phiên bản trước đó hay không. Điều này đặc biệt quan trọng đối với các ứng dụng như tóm tắt, tạo nội dung, hoặc các tác tử thông minh, nơi các phán đoán chủ quan và tính đúng đắn tinh tế đóng một vai trò trung tâm.\nKhi các tổ chức đẩy mạnh việc triển khai các mô hình này trong môi trường sản xuất, chúng tôi nhận thấy nhu cầu ngày càng tăng từ khách hàng muốn đánh giá chất lượng mô hình một cách có hệ thống, vượt qua các phương pháp đánh giá truyền thống. Các phương pháp hiện tại như đo lường độ chính xác và đánh giá dựa trên quy tắc, mặc dù hữu ích, không thể giải quyết hoàn toàn các nhu cầu đánh giá tinh tế này, đặc biệt khi các tác vụ đòi hỏi phán đoán chủ quan, hiểu biết theo ngữ cảnh, hoặc sự phù hợp với các yêu cầu kinh doanh cụ thể. Để thu hẹp khoảng cách này, LLM-như-một-giám-khảo đã nổi lên như một phương pháp đầy hứa hẹn, sử dụng khả năng lý luận của các LLM để đánh giá các mô hình khác một cách linh hoạt hơn và ở quy mô lớn.\nHôm nay, chúng tôi vui mừng giới thiệu một phương pháp toàn diện để đánh giá mô hình thông qua khả năng Amazon Nova LLM-as-a-Judge trên Amazon SageMaker AI, một dịch vụ được quản lý toàn phần của Amazon Web Services (AWS) để xây dựng, huấn luyện và triển khai các mô hình học máy (ML) ở quy mô lớn. Amazon Nova LLM-as-a-Judge được thiết kế để cung cấp các đánh giá mạnh mẽ, không thiên vị về kết quả đầu ra của AI tạo sinh trên các họ mô hình. Nova LLM-as-a-Judge có sẵn dưới dạng các quy trình công việc được tối ưu hóa trên SageMaker AI, và với nó, bạn có thể bắt đầu đánh giá hiệu suất mô hình so với các trường hợp sử dụng cụ thể của mình trong vài phút. Không giống như nhiều bộ đánh giá thể hiện sự thiên vị về mặt kiến trúc, Nova LLM-as-a-Judge đã được xác thực nghiêm ngặt để đảm bảo tính khách quan và đã đạt được hiệu suất hàng đầu trên các bộ tiêu chuẩn đánh giá chính trong khi phản ánh sát sao sở thích của con người. Với độ chính xác vượt trội và độ thiên vị tối thiểu, nó đặt ra một tiêu chuẩn mới cho việc đánh giá LLM đáng tin cậy, cấp độ sản xuất.\nKhả năng Nova LLM-as-a-Judge cung cấp các so sánh cặp đôi giữa các phiên bản mô hình, để bạn có thể tự tin đưa ra các quyết định dựa trên dữ liệu về việc cải tiến mô hình.\nCách Nova LLM-as-a-Judge được huấn luyện Nova LLM-as-a-Judge được xây dựng thông qua một quy trình huấn luyện nhiều bước bao gồm các giai đoạn huấn luyện có giám sát và học tăng cường, sử dụng các bộ dữ liệu công khai được gán nhãn với sở thích của con người. Đối với thành phần độc quyền, nhiều người gán nhãn đã độc lập đánh giá hàng ngàn ví dụ bằng cách so sánh các cặp phản hồi LLM khác nhau cho cùng một câu lệnh. Để xác minh tính nhất quán và công bằng, tất cả các gán nhãn đều trải qua các cuộc kiểm tra chất lượng nghiêm ngặt, với các phán quyết cuối cùng được hiệu chỉnh để phản ánh sự đồng thuận rộng rãi của con người thay vì một quan điểm cá nhân.\nDữ liệu huấn luyện được thiết kế để vừa đa dạng vừa mang tính đại diện. Các câu lệnh bao trùm một loạt các hạng mục, bao gồm kiến thức thực tế, sáng tạo, lập trình, toán học, các lĩnh vực chuyên ngành, và độc tính, để mô hình có thể đánh giá các kết quả đầu ra trên nhiều kịch bản thực tế. Dữ liệu huấn luyện bao gồm dữ liệu từ hơn 90 ngôn ngữ và chủ yếu bao gồm tiếng Anh, Nga, Trung, Đức, Nhật và Ý. Điều quan trọng là, một nghiên cứu về độ thiên vị nội bộ đánh giá hơn 10.000 phán đoán ưu tiên của con người so với 75 mô hình của bên thứ ba đã xác nhận rằng Amazon Nova LLM-as-a-Judge chỉ cho thấy độ thiên vị tổng hợp là 3% so với các chú thích của con người. Mặc dù đây là một thành tựu đáng kể trong việc giảm thiểu sự thiên vị có hệ thống, chúng tôi vẫn khuyến nghị thỉnh thoảng kiểm tra tại chỗ để xác thực các so sánh quan trọng.\nTrong hình dưới đây, bạn có thể thấy độ thiên vị của Nova LLM-as-a-Judge so với sở thích của con người khi đánh giá các kết quả đầu ra của Amazon Nova so với các kết quả đầu ra từ các mô hình khác. Ở đây, độ thiên vị được đo bằng sự khác biệt giữa sở thích của giám khảo và sở thích của con người qua hàng ngàn ví dụ. Giá trị dương cho thấy giám khảo hơi thiên vị các mô hình Amazon Nova, và giá trị âm cho thấy điều ngược lại. Để định lượng độ tin cậy của các ước tính này, các khoảng tin cậy 95% đã được tính toán bằng cách sử dụng sai số chuẩn cho sự khác biệt của các tỷ lệ, giả định các phân phối nhị thức độc lập.\nAmazon Nova LLM-as-a-Judge đạt được hiệu suất tiên tiến trong số các mô hình đánh giá, thể hiện sự tương đồng cao với các phán đoán của con người trên một loạt các tác vụ. Ví dụ, nó đạt độ chính xác 45% trên JudgeBench (so với 42% của Meta J1 8B) và 68% trên PPE (so với 60% của Meta J1 8B). Dữ liệu từ J1 8B của Meta được lấy từ bài viết Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning.\nNhững kết quả này làm nổi bật sức mạnh của Amazon Nova LLM-as-a-Judge trong các đánh giá liên quan đến chatbot, như được thể hiện trong bộ tiêu chuẩn PPE. Việc đo lường hiệu suất của chúng tôi tuân theo các thực tiễn tốt nhất hiện nay, báo cáo kết quả đã được đối chiếu cho các phản hồi được hoán đổi vị trí trên JudgeBench, CodeUltraFeedback, Eval Bias, và LLMBar, trong khi sử dụng kết quả một lượt cho PPE.\nMô hình Eval Bias Judge Bench LLM Bar PPE CodeUltraFeedback Nova LLM-as-a-Judge 0.76 0.45 0.67 0.68 0.64 Meta J1 8B – 0.42 – 0.60 – Nova Micro 0.56 0.37 0.55 0.6 – Trong bài đăng này, chúng tôi trình bày một phương pháp tinh gọn để triển khai các đánh giá Amazon Nova LLM-as-a-Judge bằng SageMaker AI, diễn giải các chỉ số kết quả, và áp dụng quy trình này để cải thiện các ứng dụng AI tạo sinh của bạn.\nTổng quan về quy trình công việc đánh giá Quy trình đánh giá bắt đầu bằng việc chuẩn bị một bộ dữ liệu trong đó mỗi ví dụ bao gồm một câu lệnh và hai kết quả đầu ra thay thế của mô hình. Định dạng JSONL trông như sau:\n{\n\u0026ldquo;prompt\u0026rdquo;:\u0026ldquo;Explain photosynthesis.\u0026rdquo;,\n\u0026ldquo;response_A\u0026rdquo;:\u0026ldquo;Answer A\u0026hellip;\u0026rdquo;,\n\u0026ldquo;response_B\u0026rdquo;:\u0026ldquo;Answer B\u0026hellip;\u0026rdquo;\n}\n{\n\u0026ldquo;prompt\u0026rdquo;:\u0026ldquo;Summarize the article.\u0026rdquo;,\n\u0026ldquo;response_A\u0026rdquo;:\u0026ldquo;Answer A\u0026hellip;\u0026rdquo;,\n\u0026ldquo;response_B\u0026rdquo;:\u0026ldquo;Answer B\u0026hellip;\u0026rdquo;\n}\nSau khi chuẩn bị bộ dữ liệu này, bạn sử dụng công thức đánh giá được cung cấp của SageMaker, công thức này định cấu hình chiến lược đánh giá, chỉ định mô hình nào sẽ được sử dụng làm giám khảo, và xác định các cài đặt suy luận như nhiệt độ (temperature) và top_p.\nViệc đánh giá chạy bên trong một công việc huấn luyện của SageMaker sử dụng các container Amazon Nova dựng sẵn. SageMaker AI cung cấp tài nguyên tính toán, điều phối việc đánh giá, và ghi các chỉ số đầu ra cùng các hình ảnh trực quan hóa vào Amazon Simple Storage Service (Amazon S3).\nKhi hoàn tất, bạn có thể tải xuống và phân tích kết quả, bao gồm các phân phối ưu tiên, tỷ lệ thắng, và các khoảng tin cậy.\nTìm hiểu cách hoạt động của Amazon Nova LLM-as-a-Judge Amazon Nova LLM-as-a-Judge sử dụng một phương pháp đánh giá gọi là giám khảo ưu tiên tổng thể nhị phân. Giám khảo ưu tiên tổng thể nhị phân là một phương pháp trong đó một mô hình ngôn ngữ so sánh hai kết quả đầu ra cạnh nhau và chọn ra cái tốt hơn hoặc tuyên bố hòa. Đối với mỗi ví dụ, nó đưa ra một sự ưu tiên rõ ràng. Khi bạn tổng hợp các phán đoán này trên nhiều mẫu, bạn sẽ nhận được các chỉ số như tỷ lệ thắng và khoảng tin cậy. Phương pháp này sử dụng khả năng lý luận của chính mô hình để đánh giá các phẩm chất như sự liên quan và độ rõ ràng một cách trực tiếp và nhất quán.\nMô hình giám khảo này nhằm cung cấp các ưu tiên tổng thể chung có độ trễ thấp trong các tình huống mà phản hồi chi tiết không cần thiết Kết quả đầu ra của mô hình này là một trong hai [[A\u0026gt;B]] hoặc [[B\u0026gt;A]] Các trường hợp sử dụng cho mô hình này chủ yếu là những trường hợp yêu cầu các ưu tiên cặp đôi chung, tự động và có độ trễ thấp, chẳng hạn như chấm điểm tự động để lựa chọn điểm kiểm tra (checkpoint) trong các quy trình huấn luyện Tìm hiểu các chỉ số đánh giá của Amazon Nova LLM-as-a-Judge Khi sử dụng khung Amazon Nova LLM-as-a-Judge để so sánh các kết quả đầu ra từ hai mô hình ngôn ngữ, SageMaker AI tạo ra một bộ toàn diện các chỉ số định lượng. Bạn có thể sử dụng các chỉ số này để đánh giá mô hình nào hoạt động tốt hơn và độ tin cậy của việc đánh giá. Kết quả được chia thành ba loại chính: các chỉ số ưu tiên cốt lõi, các chỉ số tin cậy thống kê, và các chỉ số sai số chuẩn.\nCác chỉ số ưu tiên cốt lõi báo cáo tần suất kết quả đầu ra của mỗi mô hình được mô hình giám khảo ưa thích. Chỉ số a_scores đếm số lượng ví dụ mà Mô hình A được ưu tiên, và b_scores đếm các trường hợp Mô hình B được chọn là tốt hơn. Chỉ số ties ghi lại các trường hợp mà mô hình giám khảo đánh giá cả hai phản hồi đều bằng nhau hoặc không thể xác định một sự ưu tiên rõ ràng. Chỉ số inference_error đếm các trường hợp mà giám khảo không thể tạo ra một phán đoán hợp lệ do dữ liệu bị định dạng sai hoặc lỗi nội bộ.\nCác chỉ số tin cậy thống kê định lượng khả năng các ưu tiên quan sát được phản ánh sự khác biệt thực sự về chất lượng mô hình thay vì biến thiên ngẫu nhiên. Chỉ số winrate báo cáo tỷ lệ trên tất cả các so sánh hợp lệ mà trong đó Mô hình B được ưa thích. lower_rate và upper_rate xác định các giới hạn dưới và trên của khoảng tin cậy 95% cho tỷ lệ thắng này. Ví dụ, một winrate là 0.75 với khoảng tin cậy từ 0.60 đến 0.85 cho thấy rằng, ngay cả khi tính đến sự không chắc chắn, Mô hình B vẫn được ưa thích hơn Mô hình A một cách nhất quán. Trường score thường khớp với số lần thắng của Mô hình B nhưng cũng có thể được tùy chỉnh cho các chiến lược đánh giá phức tạp hơn.\nCác chỉ số sai số chuẩn cung cấp một ước tính về sự không chắc chắn thống kê trong mỗi lần đếm. Chúng bao gồm a_scores_stderr, b_scores_stderr, ties_stderr, inference_error_stderr, và score_stderr. Các giá trị sai số chuẩn nhỏ hơn cho thấy kết quả đáng tin cậy hơn. Các giá trị lớn hơn có thể chỉ ra sự cần thiết phải có thêm dữ liệu đánh giá hoặc kỹ thuật câu lệnh nhất quán hơn.\nViệc diễn giải các chỉ số này đòi hỏi sự chú ý đến cả các ưu tiên quan sát được và các khoảng tin cậy:\nNếu winrate cao hơn đáng kể so với 0.5 và khoảng tin cậy không bao gồm 0.5, thì Mô hình B được ưu tiên hơn về mặt thống kê so với Mô hình A. Ngược lại, nếu winrate thấp hơn 0.5 và khoảng tin cậy hoàn toàn nằm dưới 0.5, thì Mô hình A được ưa thích hơn. Khi khoảng tin cậy chồng lấp với 0.5, kết quả là không có kết luận và nên tiến hành đánh giá thêm. Các giá trị cao trong inference_error hoặc sai số chuẩn lớn cho thấy có thể đã có vấn đề trong quá trình đánh giá, chẳng hạn như sự không nhất quán trong định dạng câu lệnh hoặc kích thước mẫu không đủ. Sau đây là một ví dụ về kết quả chỉ số từ một lần chạy đánh giá:\n{\n\u0026ldquo;a_scores\u0026rdquo;: 16.0,\n\u0026ldquo;a_scores_stderr\u0026rdquo;: 0.03,\n\u0026ldquo;b_scores\u0026rdquo;: 10.0,\n\u0026ldquo;b_scores_stderr\u0026rdquo;: 0.09,\n\u0026ldquo;ties\u0026rdquo;: 0.0,\n\u0026ldquo;ties_stderr\u0026rdquo;: 0.0,\n\u0026ldquo;inference_error\u0026rdquo;: 0.0,\n\u0026ldquo;inference_error_stderr\u0026rdquo;: 0.0,\n\u0026ldquo;score\u0026rdquo;: 10.0,\n\u0026ldquo;score_stderr\u0026rdquo;: 0.09,\n\u0026ldquo;winrate\u0026rdquo;: 0.38,\n\u0026ldquo;lower_rate\u0026rdquo;: 0.23,\n\u0026ldquo;upper_rate\u0026rdquo;: 0.56\n}\nTrong ví dụ này, Mô hình A được ưa thích 16 lần, Mô hình B được ưa thích 10 lần, và không có trường hợp hòa hoặc lỗi suy luận nào. Tỷ lệ thắng 0.38 cho thấy Mô hình B được ưa thích trong 38% các trường hợp, với khoảng tin cậy 95% dao động từ 23% đến 56%. Vì khoảng này bao gồm 0.5, kết quả này cho thấy việc đánh giá không có kết luận, và có thể cần thêm dữ liệu để làm rõ mô hình nào hoạt động tốt hơn về tổng thể.\nCác chỉ số này, được tạo tự động như một phần của quá trình đánh giá, cung cấp một nền tảng thống kê nghiêm ngặt để so sánh các mô hình và đưa ra các quyết định dựa trên dữ liệu về việc triển khai mô hình nào.\nTổng quan về giải pháp Giải pháp này trình bày cách đánh giá các mô hình AI tạo sinh trên Amazon SageMaker AI bằng cách sử dụng khả năng Nova LLM-as-a-Judge. Mã Python được cung cấp sẽ hướng dẫn bạn qua toàn bộ quy trình công việc.\nĐầu tiên, nó chuẩn bị một bộ dữ liệu bằng cách lấy mẫu các câu hỏi từ SQuAD và tạo ra các phản hồi ứng viên từ Qwen2.5 và Claude 3.7 của Anthropic. Những kết quả đầu ra này được lưu trong một tệp JSONL chứa câu lệnh và cả hai phản hồi.\nChúng tôi đã truy cập Claude 3.7 Sonnet của Anthropic trong Amazon Bedrock bằng client bedrock-runtime. Chúng tôi đã truy cập Qwen2.5 1.5B bằng một điểm cuối (endpoint) Hugging Face được lưu trữ trên SageMaker.\nTiếp theo, một Trình ước tính PyTorch (PyTorch Estimator) khởi chạy một công việc đánh giá bằng cách sử dụng một công thức Amazon Nova LLM-as-a-Judge. Công việc này chạy trên các phiên bản GPU như ml.g5.12xlarge và tạo ra các chỉ số đánh giá, bao gồm tỷ lệ thắng, khoảng tin cậy, và số lần ưu tiên. Kết quả được lưu vào Amazon S3 để phân tích.\nCuối cùng, một hàm trực quan hóa sẽ hiển thị các biểu đồ và bảng, tóm tắt mô hình nào được ưa thích hơn, mức độ ưu tiên mạnh như thế nào, và độ tin cậy của các ước tính. Thông qua phương pháp từ đầu đến cuối này, bạn có thể đánh giá các cải tiến, theo dõi các sự suy giảm hiệu suất, và đưa ra các quyết định dựa trên dữ liệu về việc triển khai các mô hình tạo sinh—tất cả mà không cần gán nhãn thủ công.\nCác điều kiện tiên quyết Bạn cần hoàn thành các điều kiện tiên quyết sau đây trước khi có thể chạy sổ tay:\nThực hiện các yêu cầu tăng hạn ngạch sau đây cho SageMaker AI. Đối với trường hợp sử dụng này, bạn cần yêu cầu tối thiểu 1 phiên bản g5.12xlarge. Trên bảng điều khiển Service Quotas, hãy yêu cầu các hạn ngạch SageMaker AI sau, 1 phiên bản G5 (g5.12xlarge) để sử dụng cho công việc huấn luyện (Tùy chọn) Bạn có thể tạo một miền Amazon SageMaker Studio (tham khảo Sử dụng thiết lập nhanh cho Amazon SageMaker AI) để truy cập các sổ tay Jupyter với vai trò nói trên. (Bạn cũng có thể sử dụng JupyterLab trong môi trường cục bộ của mình.) Tạo một vai trò AWS Identity and Access Management (IAM) với các chính sách được quản lý là AmazonSageMakerFullAccess, AmazonS3FullAccess, và AmazonBedrockFullAccess để cấp quyền truy cập cần thiết cho SageMaker AI và Amazon Bedrock để chạy các ví dụ.\nGán mối quan hệ tin cậy sau đây cho vai trò IAM của bạn:\n{\r\u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;,\n\u0026ldquo;Statement\u0026rdquo;: [\n{\n\u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;\u0026rdquo;,\n\u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;,\n\u0026ldquo;Principal\u0026rdquo;: {\n\u0026ldquo;Service\u0026rdquo;: [\n\u0026ldquo;bedrock.amazonaws.com\u0026rdquo;,\n\u0026ldquo;sagemaker.amazonaws.com\u0026rdquo;\n]\n},\n\u0026ldquo;Action\u0026rdquo;: \u0026ldquo;sts:AssumeRole\u0026rdquo;\n}\n]\n}\nSao chép (Clone) kho lưu trữ GitHub với các tài sản cho việc triển khai này. Kho lưu trữ này bao gồm một sổ tay tham chiếu đến các tài sản huấn luyện: git clone https://github.com/aws-samples/amazon-nova-samples.git\ncd customization/SageMakerTrainingJobs/Amazon-Nova-LLM-As-A-Judge/\n4. Tiếp theo, chạy sổ tay Nova Amazon-Nova-LLM-as-a-Judge-Sagemaker-AI.ipynb để bắt đầu sử dụng việc triển khai Amazon Nova LLM-as-a-Judge trên Amazon SageMaker AI.\nThiết lập mô hình Để tiến hành một cuộc đánh giá Amazon Nova LLM-as-a-Judge, bạn cần tạo ra các kết quả đầu ra từ các mô hình ứng viên mà bạn muốn so sánh. Trong dự án này, chúng tôi đã sử dụng hai phương pháp khác nhau: triển khai mô hình Qwen2.5 1.5B trên Amazon SageMaker và gọi mô hình Claude 3.7 Sonnet của Anthropic trong Amazon Bedrock. Đầu tiên, chúng tôi đã triển khai Qwen2.5 1.5B, một mô hình ngôn ngữ đa ngôn ngữ có trọng số mở, trên một điểm cuối (endpoint) SageMaker chuyên dụng. Điều này được thực hiện bằng cách sử dụng giao diện triển khai HuggingFaceModel. Để triển khai mô hình Qwen2.5 1.5B, chúng tôi đã cung cấp một tập lệnh tiện lợi để bạn gọi: python3 deploy_sm_model.py\nKhi đã được triển khai, việc suy luận có thể được thực hiện bằng một hàm trợ giúp bao bọc API dự đoán của SageMaker:\n# Khởi tạo predictor một lần\npredictor = HuggingFacePredictor(endpoint_name=\u0026ldquo;qwen25-\u0026lt;endpoint_name_here\u0026gt;\u0026rdquo;)\ndef generate_with_qwen25(prompt: str, max_tokens: int = 500, temperature: float = 0.9) -\u0026gt; str:\n\u0026quot;\u0026quot;\u0026quot;\nGửi một câu lệnh đến mô hình Qwen2.5 đã triển khai trên SageMaker và trả về phản hồi được tạo ra.\nArgs:\nprompt (str): Câu lệnh/câu hỏi đầu vào để gửi đến mô hình.\nmax_tokens (int): Số lượng token tối đa để tạo ra.\ntemperature (float): Nhiệt độ lấy mẫu cho việc tạo ra.\nReturns:\nstr: Văn bản do mô hình tạo ra.\n\u0026quot;\u0026quot;\u0026quot;\nresponse = predictor.predict({\n\u0026ldquo;inputs\u0026rdquo;: prompt,\n\u0026ldquo;parameters\u0026rdquo;: {\n\u0026ldquo;max_new_tokens\u0026rdquo;: max_tokens,\n\u0026ldquo;temperature\u0026rdquo;: temperature\n}\n})\nreturn response[0][\u0026ldquo;generated_text\u0026rdquo;]\nanswer = generate_with_qwen25(\u0026ldquo;What is the Grotto at Notre Dame?\u0026rdquo;)\nprint(answer)\nSong song đó, chúng tôi đã tích hợp mô hình Claude 3.7 Sonnet của Anthropic trong Amazon Bedrock. Amazon Bedrock cung cấp một lớp API được quản lý để truy cập các mô hình nền tảng (FM) độc quyền mà không cần quản lý cơ sở hạ tầng. Hàm tạo của Claude đã sử dụng client bedrock-runtime của AWS SDK for Python (Boto3), nhận một câu lệnh từ người dùng và trả về phần hoàn thành văn bản của mô hình:\n# Khởi tạo client Bedrock một lần\nbedrock = boto3.client(\u0026ldquo;bedrock-runtime\u0026rdquo;, region_name=\u0026ldquo;us-east-1\u0026rdquo;)\n# ID mô hình (Claude 3.7 Sonnet) thông qua Bedrock\nMODEL_ID = \u0026ldquo;us.anthropic.claude-3-7-sonnet-20250219-v1:0\u0026rdquo;\ndef generate_with_claude4(prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9) -\u0026gt; str:\n\u0026quot;\u0026quot;\u0026quot;\nGửi một câu lệnh đến mô hình Claude 4-tier qua Amazon Bedrock và trả về phản hồi được tạo ra.\nArgs:\nprompt (str): Tin nhắn người dùng hoặc câu lệnh đầu vào.\nmax_tokens (int): Số lượng token tối đa để tạo ra.\ntemperature (float): Nhiệt độ lấy mẫu cho việc tạo ra.\ntop_p (float): Lấy mẫu hạt nhân top-p.\nReturns:\nstr: Nội dung văn bản do Claude tạo ra.\n\u0026quot;\u0026quot;\u0026quot;\npayload = {\n\u0026ldquo;anthropic_version\u0026rdquo;: \u0026ldquo;bedrock-2023-05-31\u0026rdquo;,\n\u0026ldquo;messages\u0026rdquo;: [{\u0026ldquo;role\u0026rdquo;: \u0026ldquo;user\u0026rdquo;, \u0026ldquo;content\u0026rdquo;: prompt}],\n\u0026ldquo;max_tokens\u0026rdquo;: max_tokens,\n\u0026ldquo;temperature\u0026rdquo;: temperature,\n\u0026ldquo;top_p\u0026rdquo;: top_p\n}\nresponse = bedrock.invoke_model(\nmodelId=MODEL_ID,\nbody=json.dumps(payload),\ncontentType=\u0026ldquo;application/json\u0026rdquo;,\naccept=\u0026ldquo;application/json\u0026rdquo;\n)\nresponse_body = json.loads(response[\u0026lsquo;body\u0026rsquo;].read())\nreturn response_body[\u0026ldquo;content\u0026rdquo;][0][\u0026ldquo;text\u0026rdquo;]\nanswer = generate_with_claude4(\u0026ldquo;What is the Grotto at Notre Dame?\u0026rdquo;)\nprint(answer)\nKhi bạn đã tạo và kiểm tra cả hai hàm, bạn có thể chuyển sang tạo dữ liệu đánh giá cho Nova LLM-as-a-Judge.\nChuẩn bị bộ dữ liệu Để tạo một bộ dữ liệu đánh giá thực tế nhằm so sánh các mô hình Qwen và Claude, chúng tôi đã sử dụng Bộ dữ liệu Trả lời câu hỏi Stanford (SQuAD), một bộ tiêu chuẩn được áp dụng rộng rãi trong lĩnh vực hiểu ngôn ngữ tự nhiên được phân phối theo giấy phép CC BY-SA 4.0. SQuAD bao gồm hàng ngàn cặp câu hỏi-trả lời được đóng góp từ cộng đồng, bao trùm một loạt các bài viết trên Wikipedia. Bằng cách lấy mẫu từ bộ dữ liệu này, chúng tôi đảm bảo rằng các câu lệnh đánh giá của mình phản ánh các tác vụ trả lời câu hỏi thực tế, chất lượng cao, đại diện cho các ứng dụng trong thế giới thực.\nChúng tôi bắt đầu bằng cách tải một tập hợp con nhỏ các ví dụ để giữ cho quy trình công việc nhanh chóng và có thể tái tạo. Cụ thể, chúng tôi đã sử dụng thư viện datasets của Hugging Face để tải xuống và tải 20 ví dụ đầu tiên từ phần huấn luyện của SQuAD:\nfrom datasets import load_dataset\nsquad = load_dataset(\u0026ldquo;squad\u0026rdquo;, split=\u0026ldquo;train[:20]\u0026rdquo;)\nLệnh này truy xuất một lát cắt của bộ dữ liệu đầy đủ, chứa 20 mục với các trường có cấu trúc bao gồm ngữ cảnh, câu hỏi, và câu trả lời. Để xác minh nội dung và kiểm tra một ví dụ, chúng tôi đã in ra một câu hỏi mẫu và câu trả lời đúng của nó:\nprint(squad[3][\u0026ldquo;question\u0026rdquo;])\nprint(squad[3][\u0026ldquo;answers\u0026rdquo;][\u0026ldquo;text\u0026rdquo;][0])\nĐối với bộ đánh giá, chúng tôi đã chọn sáu câu hỏi đầu tiên từ tập hợp con này:\nquestions = [squad[i][\u0026ldquo;question\u0026rdquo;] for i in range(6)]\nTạo bộ dữ liệu đánh giá Amazon Nova LLM-as-a-Judge Sau khi chuẩn bị một bộ câu hỏi đánh giá từ SQuAD, chúng tôi đã tạo ra các kết quả đầu ra từ cả hai mô hình và tập hợp chúng vào một bộ dữ liệu có cấu trúc để được sử dụng bởi quy trình công việc Amazon Nova LLM-as-a-Judge. Bộ dữ liệu này đóng vai trò là đầu vào cốt lõi cho các công thức đánh giá của SageMaker AI. Để làm điều này, chúng tôi lặp qua từng câu lệnh hỏi và gọi hai hàm tạo đã được xác định trước đó:\ngenerate_with_qwen25() cho các phần hoàn thành từ mô hình Qwen2.5 được triển khai trên SageMaker generate_with_claude() cho các phần hoàn thành từ Claude 3.7 Sonnet của Anthropic trong Amazon Bedrock Đối với mỗi câu lệnh, quy trình công việc đã cố gắng tạo ra một phản hồi từ mỗi mô hình. Nếu một lệnh gọi tạo phản hồi thất bại do lỗi API, hết thời gian chờ, hoặc vấn đề khác, hệ thống đã bắt ngoại lệ và lưu trữ một thông báo lỗi rõ ràng cho biết sự thất bại. Điều này đảm bảo rằng quá trình đánh giá có thể tiến hành một cách trơn tru ngay cả khi có lỗi tạm thời:\nimport json\noutput_path = \u0026ldquo;llm_judge.jsonl\u0026rdquo;\nwith open(output_path, \u0026ldquo;w\u0026rdquo;) as f:\nfor q in questions:\ntry:\nresponse_a = generate_with_qwen25(q)\nexcept Exception as e:\nresponse_a = f\u0026quot;[Qwen2.5 generation failed: {e}]\u0026quot;\ntry: response\\_b \\= generate\\_with\\_claude4(q) except Exception as e: response\\_b \\= f\u0026quot;\\[Claude 3.7 generation failed: {e}\\]\u0026quot; row \\= { \u0026quot;prompt\u0026quot;: q, \u0026quot;response\\_A\u0026quot;: response\\_a, \u0026quot;response\\_B\u0026quot;: response\\_b } f.write(json.dumps(row) \\+ \u0026quot;\\\\n\u0026quot;) print(f\u0026quot;JSONL file created at: {output_path}\u0026quot;)\nQuy trình công việc này đã tạo ra một tệp JSON Lines có tên là llm_judge.jsonl. Mỗi dòng chứa một bản ghi đánh giá duy nhất được cấu trúc như sau:\n{\n\u0026ldquo;prompt\u0026rdquo;: \u0026ldquo;What is the capital of France?\u0026rdquo;,\n\u0026ldquo;response_A\u0026rdquo;: \u0026ldquo;The capital of France is Paris.\u0026rdquo;,\n\u0026ldquo;response_B\u0026rdquo;: \u0026ldquo;Paris is the capital city of France.\u0026rdquo;\n}\nSau đó, tải tệp llm_judge.jsonl này lên một bucket S3 mà bạn đã xác định trước:\nupload_to_s3(\n\u0026ldquo;llm_judge.jsonl\u0026rdquo;,\n\u0026ldquo;s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt;/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl\u0026rdquo;\n)\nKhởi chạy công việc đánh giá Nova LLM-as-a-Judge Sau khi chuẩn bị bộ dữ liệu và tạo công thức đánh giá, bước cuối cùng là khởi chạy công việc huấn luyện SageMaker thực hiện việc đánh giá Amazon Nova LLM-as-a-Judge. Trong quy trình công việc này, công việc huấn luyện hoạt động như một quy trình tự khép kín, được quản lý toàn phần, tải mô hình, xử lý bộ dữ liệu, và tạo ra các chỉ số đánh giá tại vị trí Amazon S3 được chỉ định của bạn.\nChúng tôi sử dụng lớp trình ước tính PyTorch từ SageMaker Python SDK để đóng gói cấu hình cho lần chạy đánh giá. Trình ước tính xác định các tài nguyên tính toán, hình ảnh container, công thức đánh giá, và các đường dẫn đầu ra để lưu trữ kết quả:\nestimator = PyTorch(\noutput_path=output_s3_uri,\nbase_job_name=job_name,\nrole=role,\ninstance_type=instance_type,\ntraining_recipe=recipe_path,\nsagemaker_session=sagemaker_session,\nimage_uri=image_uri,\ndisable_profiler=True,\ndebugger_hook_config=False,\n)\nKhi trình ước tính được cấu hình, bạn khởi tạo công việc đánh giá bằng phương thức fit(). Lệnh gọi này gửi công việc đến mặt phẳng điều khiển của SageMaker, cung cấp cụm máy tính, và bắt đầu xử lý bộ dữ liệu đánh giá:\nestimator.fit(inputs={\u0026ldquo;train\u0026rdquo;: evalInput})\nKết quả từ công việc đánh giá Amazon Nova LLM-as-a-Judge Đồ họa sau đây minh họa kết quả của công việc đánh giá Amazon Nova LLM-as-a-Judge.\nĐể giúp các chuyên gia nhanh chóng diễn giải kết quả của một cuộc đánh giá Nova LLM-as-a-Judge, chúng tôi đã tạo ra một hàm tiện ích sản xuất một hình ảnh trực quan duy nhất, toàn diện, tóm tắt các chỉ số chính. Hàm này, plot_nova_judge_results, sử dụng Matplotlib và Seaborn để hiển thị một hình ảnh với sáu bảng, mỗi bảng làm nổi bật một khía cạnh khác nhau của kết quả đánh giá.\nHàm này nhận vào từ điển chỉ số đánh giá—được tạo ra khi công việc đánh giá hoàn tất—và tạo ra các thành phần trực quan sau:\nBiểu đồ cột phân phối điểm – Cho thấy Mô hình A được ưa thích bao nhiêu lần, Mô hình B được ưa thích bao nhiêu lần, có bao nhiêu trường hợp hòa, và tần suất giám khảo không đưa ra được quyết định (lỗi suy luận). Điều này cung cấp một cảm nhận tức thì về mức độ quyết đoán của cuộc đánh giá và liệu có mô hình nào đang chiếm ưu thế hay không. Tỷ lệ thắng với khoảng tin cậy 95% – Vẽ biểu đồ tỷ lệ thắng tổng thể của Mô hình B so với Mô hình A, bao gồm một thanh lỗi phản ánh các giới hạn dưới và trên của khoảng tin cậy 95%. Một đường tham chiếu dọc tại 50% đánh dấu điểm không có sự ưu tiên. Nếu khoảng tin cậy không cắt qua đường này, bạn có thể kết luận rằng kết quả có ý nghĩa thống kê. Biểu đồ tròn về sở thích – Hiển thị trực quan tỷ lệ số lần Mô hình A, Mô hình B, hoặc không mô hình nào được ưa thích. Điều này giúp nhanh chóng hiểu được sự phân phối ưu tiên trong số các phán đoán hợp lệ. Biểu đồ cột so sánh điểm số của A và B – So sánh số lượng ưu tiên thô cho mỗi mô hình cạnh nhau. Một nhãn rõ ràng chú thích biên độ chênh lệch để nhấn mạnh mô hình nào có nhiều chiến thắng hơn. Đồng hồ đo tỷ lệ thắng – Mô tả tỷ lệ thắng dưới dạng một đồng hồ đo hình bán nguyệt với một kim chỉ vào hiệu suất của Mô hình B so với phạm vi lý thuyết 0–100%. Hình ảnh trực quan này giúp các bên liên quan không chuyên về kỹ thuật hiểu được tỷ lệ thắng trong nháy mắt. Bảng thống kê tóm tắt – Tổng hợp các chỉ số số học—bao gồm tổng số lần đánh giá, số lỗi, tỷ lệ thắng, và khoảng tin cậy—vào một bảng nhỏ gọn, sạch sẽ. Điều này giúp dễ dàng tham chiếu các giá trị số chính xác đằng sau các biểu đồ. Vì hàm này xuất ra một hình ảnh Matplotlib tiêu chuẩn, bạn có thể nhanh chóng lưu hình ảnh, hiển thị nó trong các sổ tay Jupyter, hoặc nhúng nó vào các tài liệu khác.\nDọn dẹp Hoàn thành các bước sau để dọn dẹp tài nguyên của bạn:\nXóa Điểm cuối (Endpoint) Qwen 2.5 1.5B của bạn\nimport boto3\n# Tạo một client dịch vụ SageMaker cấp thấp.\nsagemaker_client = boto3.client(\u0026lsquo;sagemaker\u0026rsquo;, region_name=\u0026lt;region\u0026gt;)\n# Xóa điểm cuối\nsagemaker_client.delete_endpoint(EndpointName=endpoint_name)\nNếu bạn đang sử dụng một sổ tay JupyterLab của SageMaker Studio, hãy tắt phiên bản sổ tay JupyterLab. Làm thế nào bạn có thể sử dụng khung đánh giá này Quy trình công việc Amazon Nova LLM-as-a-Judge cung cấp một cách thức đáng tin cậy, có thể lặp lại để so sánh hai mô hình ngôn ngữ trên dữ liệu của riêng bạn. Bạn có thể tích hợp điều này vào các quy trình lựa chọn mô hình để quyết định phiên bản nào hoạt động tốt nhất, hoặc bạn có thể lên lịch nó như một phần của việc đánh giá liên tục để phát hiện sự suy giảm hiệu suất theo thời gian.\nĐối với các đội ngũ xây dựng các hệ thống có tính tác tử hoặc chuyên biệt theo lĩnh vực, phương pháp này cung cấp cái nhìn sâu sắc hơn so với chỉ riêng các chỉ số tự động. Bởi vì toàn bộ quy trình chạy trên các công việc huấn luyện của SageMaker, nó có thể mở rộng nhanh chóng và tạo ra các báo cáo trực quan rõ ràng có thể được chia sẻ với các bên liên quan.\nKết luận Bài đăng này trình bày cách Nova LLM-as-a-Judge—một mô hình đánh giá chuyên biệt có sẵn thông qua Amazon SageMaker AI—có thể được sử dụng để đo lường một cách có hệ thống hiệu suất tương đối của các hệ thống AI tạo sinh. Hướng dẫn này chỉ ra cách chuẩn bị các bộ dữ liệu đánh giá, khởi chạy các công việc huấn luyện SageMaker AI với các công thức Nova LLM-as-a-Judge, và diễn giải các chỉ số kết quả, bao gồm tỷ lệ thắng và phân phối ưu tiên. Giải pháp SageMaker AI được quản lý toàn phần giúp đơn giản hóa quy trình này, để bạn có thể chạy các cuộc đánh giá mô hình có thể mở rộng, lặp lại và phù hợp với sở thích của con người.\nChúng tôi khuyến nghị bạn bắt đầu hành trình đánh giá LLM của mình bằng cách khám phá tài liệu và các ví dụ chính thức của Amazon Nova. Cộng đồng AWS AI/ML cung cấp các tài nguyên phong phú, bao gồm các hội thảo và hướng dẫn kỹ thuật, để hỗ trợ hành trình triển khai của bạn.\nĐể tìm hiểu thêm, hãy truy cập:\nTài liệu Amazon Nova Tổng quan về Amazon Bedrock Nova Tinh chỉnh các mô hình Amazon Nova Hướng dẫn tùy chỉnh Amazon Nova Về các tác giả Surya Kari là một Nhà khoa học dữ liệu AI tạo sinh cấp cao tại AWS, chuyên phát triển các giải pháp tận dụng các mô hình nền tảng tiên tiến. Anh có kinh nghiệm sâu rộng làm việc với các mô hình ngôn ngữ tiên tiến bao gồm DeepSeek-R1, họ Llama, và Qwen, tập trung vào việc tinh chỉnh và tối ưu hóa chúng. Chuyên môn của anh mở rộng đến việc triển khai các quy trình huấn luyện hiệu quả và các chiến lược triển khai bằng AWS SageMaker. Anh hợp tác với khách hàng để thiết kế và triển khai các giải pháp AI tạo sinh, giúp họ điều hướng việc lựa chọn mô hình, các phương pháp tinh chỉnh, và các chiến lược triển khai để đạt được hiệu suất tối ưu cho các trường hợp sử dụng cụ thể của họ.\nJoel Carlson là một Nhà khoa học ứng dụng cấp cao trong đội ngũ mô hình hóa nền tảng Amazon AGI. Anh chủ yếu làm việc về việc phát triển các phương pháp mới để cải thiện khả năng LLM-as-a-Judge của họ mô hình Nova.\nSaurabh Sahu là một nhà khoa học ứng dụng trong đội ngũ mô hình hóa Nền tảng Amazon AGI. Anh nhận bằng Tiến sĩ Kỹ thuật Điện từ Đại học Maryland College Park vào năm 2019. Anh có nền tảng về học máy đa phương thức, làm việc về nhận dạng giọng nói, phân tích tình cảm và hiểu âm thanh/video. Hiện tại, công việc của anh tập trung vào việc phát triển các công thức để cải thiện hiệu suất của các mô hình LLM-as-a-judge cho các tác vụ khác nhau.\nMorteza Ziyadi là một Quản lý Khoa học Ứng dụng tại Amazon AGI, nơi anh lãnh đạo nhiều dự án về các công thức sau huấn luyện và các mô hình ngôn ngữ lớn (Đa phương thức) trong đội ngũ mô hình hóa Nền tảng Amazon AGI. Trước khi gia nhập Amazon AGI, anh đã có bốn năm làm việc tại Microsoft Cloud và AI, nơi anh lãnh đạo các dự án tập trung vào việc phát triển các mô hình tạo mã từ ngôn ngữ tự nhiên cho các sản phẩm khác nhau. Anh cũng đã từng là giảng viên thỉnh giảng tại Đại học Northeastern. Anh nhận bằng Tiến sĩ từ Đại học Nam California (USC) vào năm 2017 và từ đó đã tích cực tham gia với tư cách là người tổ chức hội thảo và người phản biện cho nhiều hội nghị NLP, Thị giác máy tính và học máy.\nPradeep Natarajan là một Nhà khoa học chính cấp cao trong đội ngũ mô hình hóa Nền tảng Amazon AGI, làm việc về các công thức sau huấn luyện và các mô hình ngôn ngữ lớn Đa phương thức. Anh có hơn 20 năm kinh nghiệm trong việc phát triển và ra mắt nhiều hệ thống học máy quy mô lớn. Anh có bằng Tiến sĩ Khoa học Máy tính từ Đại học Nam California.\nMichael Cai là một Kỹ sư phần mềm trong Đội ngũ Tùy chỉnh Amazon AGI, hỗ trợ việc phát triển các giải pháp đánh giá. Anh nhận bằng Thạc sĩ Khoa học Máy tính từ Đại học New York vào năm 2024. Trong thời gian rảnh, anh thích in 3D và khám phá công nghệ đổi mới.\n"
},
{
	"uri": "/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey – Project Plan [Project team] – [University] – [Project Name]\n[Date]\nTable of Contents BACKGROUND and motivation\n1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations Activities AND Deliverables\n3.1 Activities and deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN BY SERVICES\nTEAM\nRESOURCES \u0026amp; COST ESTIMATES\nACCEPTANCE\nBACKGROUND AND MOTIVATION 1.1 Executive Summary Organization faces HR evaluation inefficiencies due to manual data handling, lack of transparency in evaluation processes and metrics tracking.\nInsightHR delivers HR automation through flexible evaluation management, automated scoring, and AI insights. AWS provides serverless scalability, cost efficiency, security for sensitive data, AI Chatbot via Bedrock, and rapid deployment.\nCustom KPI, automated performance scoring, multi-level dashboards, AI assistant for natural-language queries, automated notifications, role-based access (Admin/Manager/Employee), multi-tenant support.\nEnd-to-end delivery including Well-Architected design, serverless backend (Lambda, DynamoDB, API Gateway), frontend (S3 + CloudFront), authentication/security (Cognito, IAM), KPI/formula builder, AI chatbot (Bedrock + Lambda query data from info tables), notifications (SNS, SES), CI/CD, monitoring, and knowledge transfer.\n1.2 Project Success Criteria Success is defined by demonstrating a functional MVP that proves the platform\u0026rsquo;s capability to automate HR evaluations and deliver measurable business value.\n1. Functional Criteria:\nAuthentication with role-based access (Admin/HR, Manager, Employee) HR creates custom KPIs without technical support CSV upload triggers automated Lambda scoring Dashboard displays individual/team performance with charts AI chatbot answers natural language queries from DynamoDB data SES sends automated email notifications 2. Technical Criteria:\n99.9%+ uptime \u0026lt;300ms API latency (95th percentile) 95%+ scoring accuracy vs manual calculations 90%+ AI response relevance Zero critical security vulnerabilities 3. Performance \u0026amp; Cost:\n~$33.14/month AWS cost End-to-end workflow (upload → score → visualize) completes in \u0026lt;5 minutes 4. Business Impact:\nDemonstrates 60%+ HR time reduction potential Non-technical users operate KPI builder and chatbot independently 5. Delivery:\nWeek 8: MVP (authentication, KPI/formula management, scoring, basic dashboard) Week 12: Full features (chatbot, notifications, advanced dashboard) 1.3 Assumptions 1. Assumptions:\nThe current AWS cost estimate of approximately $33.14/month is accurate for the projected initial load and usage. The required data format and mapping logic for employee performance data can be clearly defined and provided by the HR team for the automated scoring engine. The Large Language Model provided by Amazon Bedrock to support HR. The automated scoring system is trained locally. The technical evaluation files of each team are assessed according to the companies\u0026rsquo; criteria and must follow the format provided by the customer. 2. Constraints:\nThe project delivery must adhere to the 12-week timeline utilizing the Agile Scrum framework. The solution must be built entirely on serverless AWS services to meet the objectives of scalability, cost efficiency, and reduced operational overhead. The final production AWS cost must remain around the ~$33.14/month target. 3. Risks:\nData Security/Compliance: Failure to fully understand or implement all of the customer\u0026rsquo;s specific regulatory control validation requirements could impact the project\u0026rsquo;s ability to meet security objectives. Feature Creep: Requests for features identified as \u0026ldquo;Out of Scope\u0026rdquo; could derail the 12-week MVP delivery timeline. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The InsightHR platform is built on a serverless architecture using AWS services, providing scalability, cost-effectiveness, and high availability. The architecture includes:\n1. Frontend \u0026amp; Content Delivery:\nAmazon S3: Hosts the static website and stores user-uploaded files (CSV, AI models). S3 Vector to store vectors (embeddings for text-data), S3 Standard for storing raw documents. Amazon CloudFront: Distributes static and dynamic content globally with low latency. 2. Backend \u0026amp; Compute:\nAWS Lambda: Executes all business logic, including authentication, custom scoring, and chatbot functions. Amazon API Gateway: Manages APIs as the communication gateway between frontend and backend. 3. Data Storage:\nAmazon DynamoDB: Stores structured data such as user/employee information, company KPIs, scoring formulas, and performance evaluation results. 4. AI \u0026amp; Machine Learning:\nAmazon Bedrock: Provides Large Language Models (LLMs) for the HR assistant chatbot. The ML Model system is trained locally for scoring function. 5. Security \u0026amp; Identity:\nAmazon Cognito: Manages user authentication, registration, and identity workflows. AWS IAM: Manages access control and permissions for AWS services. AWS KMS: Encrypts sensitive data in DynamoDB and S3. 6. Monitoring \u0026amp; Notifications:\nAmazon CloudWatch \u0026amp; CloudWatch Logs: Monitors Lambda functions, API Gateway, and database access. Amazon SNS: Sends notifications (e.g., reminders, result notifications) to employees. 7. Architecture Benefits:\nServerless: No server management and automatic scaling. Cost-Effective: Mostly pay-as-you-go services. High Availability: Built-in redundancy across AWS regions. Scalable: Can handle growth from small teams to large enterprises. Flexible: Easy to modify and extend functionality. 8. Proposed Architecture Diagram:\n9. Tools Proposed for This Project:\nAmazon CloudFront: For global content delivery and caching of static and dynamic web content. Amazon S3: To host static web assets and store documents, vector embeddings, and other files processed by the system. Amazon API Gateway: To provide a secure RESTful interface, acting as the communication layer between frontend clients and backend services. AWS Lambda: To run backend business logic including user dashboard, auto scoring, HR assistant, and data management workflows. Amazon DynamoDB: To store application data such as user information, HR records, scoring results, and vector metadata with low-latency performance. Amazon Cognito: To manage user authentication, authorization, registration, MFA, and secure access to APIs and frontend applications. AWS Identity and Access Management (IAM): To define fine-grained access policies and control permissions between services and users. AWS Key Management Service (KMS): To manage encryption keys used for securing sensitive data stored in S3, DynamoDB, and logs. Amazon ECR: To store containerized model assets and application dependencies in a secure and version-controlled repository. Amazon Bedrock / Large Language Model (LLM): To provide AI capabilities for chat, data extraction, summarisation, and intelligent HR workflows. Amazon Simple Email Service (SES): To send automated email notifications such as onboarding alerts and HR communications. Amazon Simple Notification Service (SNS): To publish notifications and trigger downstream processes; integrates with email, SMS, and microservices. Amazon CloudWatch \u0026amp; CloudWatch Logs: For monitoring performance, logging, tracing, and operational troubleshooting across Lambda, API Gateway, and AI components. 2.2 Technical Plan The partner will develop automated deployment scripts using AWS CloudFormation and Infrastructure as Code (IaC) practices.\nThis will allow for quick and repeatable deployments into AWS accounts. Some additional configurations such as WAF rules on CloudFront for enhanced security may require approval and will follow standard DevOps change management processes.\nApplication Feature Implementation:\n1. Authentication \u0026amp; Security Module\nUser Management: Cognito manages user lifecycle Registration, login, password reset workflows Access Control: IAM and RBAC enforce role-based permissions Admin/HR, Manager, and Employee access levels API Security: API Gateway implements JWT-protected endpoints Token validation before Lambda processing 2. Administration Module (HR Panel)\nKPI Management: HR creates, edits, and deletes custom metrics Examples: Tasks Completed, Code Quality, Customer Satisfaction Definitions stored in DynamoDB Auto scoring by employee\u0026rsquo;s technical score for each team with ML model. 3. Core User Functions\nData Upload \u0026amp; Mapping: Upload performance data files (CSV) to DynamoDB Scoring Engine: Lambda triggered on upload Retrieves active formula from DynamoDB Calculates employee scores Stores results in DynamoDB Flow: Upload → Validate → Map → Calculate → Store Dashboard: Visualize individual and department performance Line graphs, bar charts, trend analysis AI Chatbot: Bedrock (LLM) integration Natural language queries (e.g., \u0026ldquo;Summarize Team A Q4 performance\u0026rdquo;) Queries and summarizes DynamoDB data Notifications: SES sends automated alerts Performance milestones, review reminders, custom triggers 2.3 Project Plan The partner will adopt the Agile Scrum framework over 12 one-week sprints totaling a 12-week delivery timeline.\n1. Team Responsibilities\nProduct Owner: Prioritizes backlog (KPIs, formulas, analytics) Final authority on feature acceptance Development Team: Implements Cognito authentication Builds admin portal and formula builder Develops scoring engine and dashboard Integrates Bedrock chatbot and SNS with SES notifications via Email. QA Personnel: Conducts functional, performance, and security testing Facilitates UAT Ensures compliance and quality standards 2. Communication Cadences\nDaily Standups (30 min - 1 hr): Progress review and blocker identification Retrospectives (Weekly, 1 hr): Process improvement and delivery optimization Executive Updates (Weekly): Written reports on progress, risks, KPIs, roadmap Leadership decisions required 3. Knowledge Transfer\nSessions conducted by the development team covering AWS serverless fundamentals KPI and formula configuration Data workflows and column-mapping Dashboard navigation and analytics System monitoring (CloudWatch, Cognito, DynamoDB) 2.4 Security Considerations The partner will implement AWS security best practices based on the Well-Architected Framework, prioritizing protection of sensitive HR data while ensuring high operational availability. Security implementation covers five key categories:\n1. Access Control\nCognito manages user identities Enforces strong password policies and MFA support IAM implements RBAC Admin/HR access Admin Panel and KPI/Formula configurations Employees view only their own performance data API Gateway validates JWT tokens Cognito-issued tokens verified before Lambda processing 2. Infrastructure Security\nServerless architecture reduces attack surface No OS or server patching required Lambda functions communicate via private AWS networks Only necessary endpoints exposed through API Gateway 3. Data Protection\nKMS encrypts data at rest DynamoDB and S3 encrypted Data unusable without decryption keys TLS/SSL (HTTPS) encrypts data in transit All frontend-backend communication secured 4. Detection \u0026amp; Monitoring\nCloudWatch Logs captures execution details Lambda and API Gateway activity logged Real-time monitoring and anomaly detection enabled AWS Config tracks configuration changes Ensures resource compliance with security objectives 5. Incident Management\nCloudWatch Alarms trigger automated alerts via SES Failed login threshold breaches Lambda resource anomalies Security Hub provides consolidated security view Unified compliance findings across AWS environment Simplifies incident identification and response AWS CloudTrail and AWS Config will be configured for continuous monitoring of activities and compliance status of resources. The customer will share their regulatory control validation requirements as inputs for the partner to ensure all security objectives are met.\nACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables NOTE: Some Project Phases overlap each other.\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Phase 1: Foundation \u0026amp; Scoring Model Week 1-8 • Personal infrastructure architecture research • Data generation for local model training • Scoring model build • Finalized personal architecture diagram • Ready dataset for Local Model training • Scoring Model MVP (Minimum Viable Product) 80 Phase 2: Project Setup \u0026amp; Dashboard Week 9-10 • Project Setup with basic functions: IAM Role, CRUD function, Static web • Web UI Demo • Implement Dashboard • Fix Model • Basic IAM Roles configured • Operational CRUD functions • Static website deployed (S3/CloudFront) • Web UI Demo completed • Dashboard displaying data implemented 40 Phase 3: AI Agent \u0026amp; Absence Mgmt Week 11 • Building Bedrock Agent • Implement Absent managing • Bedrock Agent built • Operational Absence tracking workflow implemented 15 Phase 4: Integration, Testing \u0026amp; Handover Week 12 • Implement Chatbot into App • Testing and set up Monitoring • Chatbot integrated into the application • Functional, Performance, and Security Testing completed • Monitoring (CloudWatch) configured and operational • Project Completion Report \u0026amp; Post-implementation support plan delivered 15 3.2 Out Of Scope 1. AI Enhancements\nAI-Powered Insights:\nWhen sufficient data is available, develop AI models capable of: Chatbot accesses database directly and retrieves prompt window -\u0026gt; Price a lot of tokens and high lock, future can be optimized by other ways Identifying performance patterns across teams and departments Predicting HR risks (e.g., turnover likelihood, burnout indicators) Recommending personalized development plans Detecting anomalies in performance data Suggesting optimal team compositions Machine Learning Features:\nPredictive analytics for workforce planning Sentiment analysis from employee feedback Automated skill gap analysis Performance trend forecasting 2. Public API Development\nAPI Ecosystem:\nBuild a comprehensive API set allowing other internal business systems to automatically push performance data into InsightHR. Integration Targets:\nProject management tools (Jira, Asana, Monday.com) CRM systems (Salesforce, HubSpot) Time tracking software (Toggl, Harvest) Communication platforms (Slack, Microsoft Teams) Code repositories (GitHub, GitLab, Bitbucket) Benefits:\nTransform InsightHR into a central HR data processing hub Create a synchronized and comprehensive management ecosystem Eliminate manual data entry Real-time performance tracking 3. Advanced Features\nMobile Applications:\niOS and Android native apps Push notifications Offline capabilities Mobile-optimized dashboards DynamoDB back up Advanced Analytics:\nPredictive modeling Benchmarking across industries Custom report builder Data export and API for third-party tools Collaboration Features:\nPeer review systems 360-degree feedback Goal setting and tracking Performance improvement plans Compliance \u0026amp; Governance:\nAudit trails Compliance reporting Data retention policies Advanced access controls 3.3 Path To Production This document outlines the current production architecture and operational status for the InsightHR platform deployment. The platform is fully live in the ap-southeast-1 (Singapore) region.\n1. Platform Architecture and Access\nPublic URL: https://insight-hr.io.vn AWS Region: ap-southeast-1 (Singapore) Frontend: React application hosted on S3 (insighthr-web-app-sg) with CloudFront HTTPS distribution. Backend: 8 Lambda function groups accessed via API Gateway REST API. Database: DynamoDB tables configured with On-Demand capacity. 6 tables for each team Employee information table History score table Absent table Account managing tables Authentication: Cognito User Pool. AI/Chatbot: Amazon Bedrock (Claude 3 Haiku) integration for conversation history and knowledge base to enhance models. 2. Live Production Features\nThe following core features have been successfully deployed and are operational:\nAuthentication: Full support for email/password login, password reset workflows. User Management: Complete CRUD functionality, including bulk import and role-based access. Employee Management: Full support for 300+ employees and bulk operations. Performance Score Management: Management of 900+ quarterly scores and calendar-based viewing. Attendance Management: Processing of 9,300+ records, including check-in/check-out kiosk functionality and auto-absence marking. Performance Dashboard: Live charts, trend analysis, live clock, and CSV export capabilities. AI Chatbot: Bedrock integration with conversation history enabled. 3. Deployment and Verification Process\nThe standard, repeatable deployment workflow ensures rapid and verifiable updates to the production site:\nBuild: npm run build creates the optimized production asset bundle. Test: npm run preview validates the built bundle locally prior to deployment. Deploy: aws s3 sync dist/ s3://insighthr-web-app-sg --region ap-southeast-1 pushes assets to the S3 bucket. Invalidate: aws cloudfront create-invalidation --distribution-id E3MHW5VALWTOCI --paths \u0026quot;/*\u0026quot; clears the CloudFront CDN cache. Verify: Full feature testing is performed on the live public URL. 4. Remaining Production Enhancements\nThe platform is in the final phases of enhancement before full stabilization, with key items planned or in progress:\nPage Integration (In Progress)\nConsolidate all administrative page navigation. Verify all features are accessible from the main menu. Test role-based routing across all pages. Fix any integration bugs. Polish and Final Deployment (Planned)\nImplement comprehensive error handling and input validation. Refine responsive design for full mobile compatibility. Conduct dedicated Security testing (penetration testing, vulnerability scanning). Execute Load testing for scalability validation. Develop user documentation and training materials. Perform final production hardening procedures. Monitoring and Scalability Strategy\nActive Monitoring: CloudWatch Logs are enabled for all Lambda functions and API Gateway endpoints, along with CloudWatch Metrics for performance tracking. Planned Alarms: CloudWatch Alarms and SNS notifications are planned for critical error rates and latency. Scalability: Achieved via Serverless Architecture (DynamoDB On-Demand, Lambda, CloudFront CDN). Disaster Recovery: DynamoDB Point-in-Time Recovery and S3 Versioning are planned to be enabled for critical data/assets. Lambda code is stored in version control for rapid redeployment. EXPECTED AWS COST BREAKDOWN BY SERVICES AWS Service Monthly Estimated Cost (USD) Amazon Bedrock $21.61 AWS Lambda $3.75 Amazon Simple Email Service (SES) $2.25 Amazon DynamoDB $1.52 Amazon Simple Storage Service $0.46 Amazon CloudWatch $0.80 Amazon API Gateway $0.06 Amazon CloudFront $0.00 Amazon Cognito $0.00 Amazon EventBridge $0.00 Amazon IAM $1.60 Amazon KMS $1.03 TOTAL MONTHLY COST $33.14 TOTAL YEARLY COST $397.79 TEAM Name Task Role Email / Contact Info Bùi Tấn Phát Dashboard, Manage Employee, Support, Content check Leader btfat3103@gmail.com Nguyễn Ngọc Long CRUD, Config Network / API Gateway, Test function, Slide Member nguyenngoclong216@gmail.com Đặng Nguyễn Minh Duy Database, CloudWatch / CloudLogs, Paper, Slide Member dangnguyenminhduy11b08@gmail.com Đỗ Đăng Khoa Log In/ Registration / Forget Password, UI / UX - Static Web, Paper Member khoado7577@gmail.com Nguyễn Huỳnh Thiên Quang Auto Scoring, AI Assistant, Slide Member quangkootenhatvutru@gmail.com RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Full-Stack Developers [2] React frontend, Python Lambda backend, API integration $66 Cloud Engineers [3] AWS infrastructure setup, deployment automation, monitoring $66 Other (Please specify) Estimated platform consumption (Lambda, DynamoDB, Bedrock). Paper and present material $0.01 NOTE: Project Phase durations overlap each other.\nProject Phase Duration Man-Days Other (Please specify) Estimated Cost Phase 1: Foundation \u0026amp; Scoring Model 8 Weeks 80 - $42,246.40 (80 x $528.08) Phase 2: Project Setup \u0026amp; Dashboard 2 Weeks 40 - $21,123.20 (40 x $528.08) Phase 3: AI Agent \u0026amp; Absence Mgmt 1 Week 15 - $7,921.20 (15 x $528.08) Phase 4: Integration, Testing \u0026amp; Handover 1 Week 15 - $7,921.20 (15 x $528.08) Total Hours 12 Weeks 150 Man-Days $79,212.00 Cost Contribution distribution between Partner, Customer, AWS.\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 200 100 ACCEPTANCE 1. Project Acceptance Criteria\nThe InsightHR platform will be considered complete and accepted when the following criteria are met.\n2. Completed Deliverables\nAll major features implemented and deployed to production User and employee management with bulk operations Performance score management with calendar view Attendance system with auto-absence marking Interactive dashboard with live clock AI chatbot with Bedrock integration 3. Key Metrics Achieved\n300+ user accounts 300 employee records across 5 departments 900+ performance scores tracked 9,300+ attendance records AWS monthly cost: ~$33.14 System uptime: 99.9%+ Zero critical security vulnerabilities 4. Acceptance Status\nCurrent Status: Application deployed in cloudfront Production URL: https://d2z6tht6rq32uy.cloudfront.net 5. Next Steps\nMinor bug fixing and feature updates Conduct user acceptance testing Provide knowledge transfer and training "
},
{
	"uri": "/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives Set up development environment with Linux Fedora for AWS services Attend AWS Cloud Day 2025 and learn about new AWS technologies Establish team workflow and project management processes Apply AI-DLC (AI Driven Development Lifecycle) methodology with Kiro IDE Explore AWS S3 buckets and basic AWS services Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material 1 - Set up Linux Fedora Environment for AWS services - Troubleshooting Fedora installation and configuration - Initial team planning about different AWS services 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 2 - Continue Fedora environment configuration - Register for AWS Cloud Day 2025 (18/Sep/2025) - Team planning and coordination for upcoming activities 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Team planning and workflow setup - Set up team project management processes - Discuss AWS services: VPC, IAM roles, AWS free tier missions 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - AWS Cloud Day 2025: + Explore AWS technology and services + Learn about Amazon Nova Act + Learn about Kiro AI Agent + Learn about AI-DLC methodology + Networking and gathering with AWS community 18/09/2025 18/09/2025 AWS Cloud Day 2025 Event 5 - Apply AI-DLC methodology to create POCs in Kiro IDE - Install Kiro IDE on Windows platform - Test autonomous agentic AI workflow 19/09/2025 19/09/2025 Kiro IDE Documentation 6 - Continue testing Kiro IDE capabilities - Develop prompts for Kiro AI Agent - Refine AI-DLC workflow processes 20/09/2025 20/09/2025 Kiro IDE Documentation Week 2 Achievements Development Environment Setup: Successfully installed and configured Linux Fedora for AWS development Explored Fedora Linux features including themes, customization, and applications Troubleshot and optimized Fedora environment for development work Linux Fedora development environment configured for AWS services\nAWS Cloud Day 2025 Participation: Attended AWS Cloud Day 2025 on September 18, 2025 Learned about new AWS technologies and services: Amazon Nova Act - New AWS AI service Kiro AI Agent - Autonomous AI development assistant AI-DLC (AI Driven Development Lifecycle) - New SDLC methodology Networking with AWS professionals and community members AWS Cloud Day 2025 - Learning about new AWS technologies\nTeam Collaboration:\nEstablished team workflow and project management processes Planned and discussed various AWS services including VPC, IAM roles Coordinated on AWS free tier missions and resource sharing Prepared for proposal and workshop implementation AI-DLC Methodology Application:\nSuccessfully installed Kiro IDE on Windows platform Applied AI-DLC methodology to create proof-of-concepts Tested autonomous agentic AI workflow capabilities Developed multiple prompts for effective AI agent interaction Kiro IDE setup and AI-DLC workflow testing\nAWS Services Exploration:\nLearned how to work with S3 buckets Explored basic AWS service architecture Understood AWS free tier limitations and best practices Development Tools and Automation:\nCloned multiple FCJ (First Cloud Journey) repositories for reference Developed auto-register Chromium extension for daily office registration Troubleshot and optimized the extension for best performance Learned Hugo static site generator for documentation Auto-register Chromium extension for office registration automation\nHugo static site generator configuration\nGitHub and Version Control: Cloned and studied multiple FCJ repositories Practiced Git workflows and version control best practices Challenges Faced Minimal difficulties encountered during Week 2 Successfully resolved Fedora Linux installation and configuration issues through troubleshooting Key Learnings New Technologies:\nAmazon Nova Act capabilities and use cases AI-DLC (AI Driven Development Lifecycle) methodology and its application in modern software development Kiro AI Agent features and autonomous workflow capabilities AWS Services:\nS3 bucket management and best practices Team collaboration on AWS free tier resources AWS service planning for proposals and workshops Development Skills:\nLinux Fedora environment management Hugo static site generator Chromium extension development AI-driven development workflows Next Week Goals Continue exploring AWS services in depth Apply AI-DLC methodology to more complex projects Prepare detailed proposal for workshop implementation Deepen understanding of VPC and IAM roles "
},
{
	"uri": "/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AI-Driven Development Life Cycle Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Friday, October 3, 2025\nLearning Report: \u0026ldquo;AI-Driven Development Life Cycle\u0026rdquo; Event Objectives Understand the paradigm shift from traditional software development to AI-Driven Development Lifecycle (AI-DLC) Explore how AI transforms each phase of the software development process Learn practical applications of Amazon Q Developer and Kiro tools Discover strategies for integrating AI into existing development workflows Analyze real-world case studies of AI-accelerated software implementation Speaker List Binh Tran - Senior Solutions Architect, AWS AWS Expert Team - Specialists in AI-powered development tools Key Content Highlights 1. The AI-DLC Paradigm Shift Traditional vs AI-Driven: Understanding the fundamental differences between conventional SDLC and AI-DLC approaches AI as Central Collaborator: Unlike traditional methods that retrofit AI as an assistant, AI-DLC integrates AI-powered execution with human oversight throughout the entire lifecycle Core Benefits: Drastically improved development speed, enhanced code quality, and accelerated innovation cycles 2. Three Phases of AI-DLC Phase 1: Inception\nContext Definition: AI assists in gathering and organizing project requirements User Story Generation: Automated creation and refinement of user stories Work Planning: AI-powered breakdown of features into manageable work units Estimation: Intelligent effort estimation based on historical data and complexity analysis Phase 2: Construction\nCode Generation: AI-assisted coding with context-aware suggestions Automated Testing: AI generates comprehensive test suites including unit, integration, and edge cases Architecture Enhancement: Continuous architectural improvements suggested by AI Infrastructure as Code: Automated IaC generation and deployment scripts Quality Assurance: AI-powered code reviews and security scanning Phase 3: Operation\nDeployment Automation: AI-orchestrated deployment to production environments Incident Management: Intelligent incident detection, diagnosis, and resolution suggestions Performance Optimization: Continuous monitoring and AI-recommended optimizations Feedback Loop: AI analyzes production metrics to inform future development cycles 3. Amazon Q Developer Capabilities SDLC Automation: End-to-end support from planning through maintenance Code Transformation: Automated upgrades for Java, .NET modernization Multi-Platform Integration: Seamless integration with AWS Console, IDE, CLI, and DevSecOps platforms Intelligent Collaboration: Acts as an AI pair programmer understanding complex codebases Documentation Generation: Automatically creates comprehensive documentation and unit tests Security Enhancement: Built-in security scanning and vulnerability detection 4. Kiro Tool Demonstration Development Acceleration: Real-time code suggestions and completions Context Awareness: Understanding project structure and coding patterns Workflow Integration: Seamless integration into existing development environments Productivity Metrics: Measurable improvements in development velocity What I Learned Mindset Transformation AI as Partner, Not Tool: The most successful AI-DLC implementations treat AI as an intelligent collaborator rather than just another development tool Human Oversight Remains Critical: While AI accelerates execution, human judgment guides strategic decisions and validates AI outputs Continuous Learning: AI systems improve over time by learning from team patterns and preferences Technical Implementation Incremental Adoption: Organizations can gradually integrate AI-DLC practices without disrupting existing workflows Tool Selection: Understanding when to use Amazon Q Developer vs other AI coding assistants based on specific needs Quality Maintenance: AI-generated code requires proper review processes to maintain standards Business Impact Speed to Market: AI-DLC can reduce development cycles by 40-60% for suitable projects Code Quality: Automated testing and review processes lead to more robust applications Developer Experience: Developers can focus on creative problem-solving rather than repetitive tasks Application to Work Pilot Project: Select a small internal project to experiment with AI-DLC methodology Integrate Amazon Q: Begin using Amazon Q Developer in daily coding tasks to experience productivity gains Establish Guidelines: Create team guidelines for AI-assisted development including code review standards Measure Impact: Track metrics like development velocity, bug rates, and time-to-deployment before and after AI adoption Training Program: Develop internal training to help team members effectively leverage AI development tools Event Experience The workshop provided a comprehensive and practical view of how AI is fundamentally transforming software development.\nParadigm Shift Understanding The session clearly articulated the difference between \u0026ldquo;AI-assisted\u0026rdquo; and \u0026ldquo;AI-driven\u0026rdquo; development, helping me understand this is not just about better tools but a fundamental methodology change Real-world examples demonstrated how leading organizations are achieving significant productivity gains through AI-DLC adoption Hands-on Demonstrations Amazon Q Developer Demo: Watching live demonstrations of code generation, transformation, and documentation creation was eye-opening Kiro Tool Walkthrough: Seeing how Kiro integrates into the development workflow provided practical insights for immediate application The demos showed both the capabilities and limitations of current AI development tools, setting realistic expectations Strategic Insights Understanding the three-phase AI-DLC model (Inception, Construction, Operation) provides a clear framework for implementation The emphasis on maintaining human oversight while leveraging AI execution struck the right balance between automation and control Discussion of change management strategies for introducing AI-DLC to development teams was particularly valuable Networking and Discussion Conversations with other attendees revealed various approaches to AI adoption across different organizations Q\u0026amp;A sessions addressed practical concerns about code quality, security, and team adaptation Shared experiences highlighted both successes and challenges in AI-DLC implementation Some photos from the event Add your event photos here Overall, this event provided both the conceptual framework and practical tools needed to begin transforming our development processes with AI-DLC methodology, positioning our team to significantly accelerate delivery while maintaining quality standards.\n"
},
{
	"uri": "/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Tối ưu hóa hiệu suất băng thông hợp nhất thu nhập cố định: Kiểm tra độ trễ cho dữ liệu thị trường trái phiếu bởi Neil Ryan, George Beasley, và Vivian Lai vào ngày 17 THÁNG 7 NĂM 2025 trong Amazon EC2, Amazon Elastic Kubernetes Service, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon RDS, Dịch vụ tài chính, Các ngành Đường dẫn cố định Bình luận Chia sẻ\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Bondtape.png)\nTrong thế giới nhịp độ nhanh của thị trường tài chính, mỗi mili giây đều có giá trị. Đối với các Nhà cung cấp Băng thông Hợp nhất Thu nhập Cố định (CTP) (sắp được lựa chọn) sẽ bắt đầu hoạt động vào đầu năm 2026, việc duy trì độ trễ thấp và thông lượng cao để cung cấp thông tin định giá kịp thời và chính xác là rất quan trọng.\nBondtape Bondtape là sự hợp tác giữa FINBOURNE Technology và Propellant.digital. Họ sẽ cùng nhau cung cấp Băng thông Hợp nhất (CT) trái phiếu đã được thị trường chứng minh, đáng tin cậy và có thể mở rộng tại Vương quốc Anh, đánh dấu một bước quan trọng hướng tới việc cải thiện tính minh bạch và hiệu quả trên thị trường thu nhập cố định toàn cầu. Sự hợp tác này sẽ tận dụng công nghệ dữ liệu doanh nghiệp của FINBOURNE, kết hợp với khung phân tích của Propellant, để cung cấp một giải pháp doanh nghiệp toàn diện.\nSử dụng nền tảng của FINBOURNE, Bondtape đã phát triển một nguyên mẫu cho giải pháp CTP của họ, được thiết kế để kiểm tra chất lượng, hài hòa hóa và phổ biến dữ liệu định giá trái phiếu một cách hiệu quả trong thời gian thực. Là một phần của bài tập, họ đã triển khai một bộ dữ liệu giao dịch đầy đủ và một bộ công cụ kiểm tra, mà sau đó Bondtape đã tiến hành kiểm tra độ trễ toàn diện.\nThách thức: Cân bằng giữa Tốc độ và Sự ổn định Mục tiêu là đánh giá cách hệ thống xử lý các cường độ tải khác nhau và liệu các điều kiện này có bất kỳ tác động có thể đo lường nào đến độ trễ xử lý hay không. Việc kiểm tra là rất quan trọng để đảm bảo rằng nguyên mẫu duy trì hiệu suất nhất quán dưới cả áp lực ngắn hạn và hoạt động kéo dài, mô phỏng các kịch bản trong thế giới thực. Điều quan trọng cần lưu ý là thời gian trong bài đăng trên blog này chỉ mang tính chất tham khảo, vì nguyên mẫu sẽ nhận được các cải tiến từ việc tinh chỉnh hiệu suất và cải thiện làm mịn khối lượng công việc được lên kế hoạch như một phần của giai đoạn tiền vận hành CTP.\nPhương pháp tiếp cận của Bondtape: Nền tảng và Môi trường kiểm tra do AWS cung cấp Nền tảng đám mây gốc của Bondtape chạy trên hai Vùng sẵn sàng cho các CT của Vương quốc Anh. Kiến trúc này phân phối tải và dữ liệu một cách hiệu quả để đảm bảo khả năng phục hồi không có thời gian chết trước sự cố của thành phần, phần cứng hoặc toàn bộ Vùng sẵn sàng. Cũng cần lưu ý rằng một \u0026lsquo;khu vực\u0026rsquo; theo thuật ngữ của AWS (ví dụ: London) bao gồm 3 hoặc nhiều Vùng sẵn sàng (AZ). Các Vùng sẵn sàng này cách nhau ít nhất 100 km và mỗi vùng đều có nguồn điện, hệ thống làm mát và kết nối dự phòng riêng.\nCác AZ cuối cùng cho CTP sẽ được tối ưu hóa cho cả độ trễ và các yếu tố môi trường. Tất cả kết nối vào và ra khỏi dịch vụ sẽ được cấu hình để có khả năng phục hồi, với tính dự phòng và chuyển đổi dự phòng tự động để giảm thiểu các kịch bản có khả năng gây gián đoạn. Bondtape được xây dựng trên cùng công nghệ nền tảng với nền tảng LUSID của FINBOURNE, xử lý trung bình 10 triệu yêu cầu xử lý dữ liệu hàng ngày (3,4 tỷ vào năm 2024) với độ tin cậy hàng đầu trong ngành.\nTận dụng khả năng mở rộng và tính linh hoạt của AWS, Bondtape đã thiết lập môi trường nguyên mẫu thử nghiệm của họ với cấu hình sau:\nMột nhóm các phiên bản Amazon Elastic Compute Cloud (EC2), được điều phối bằng Amazon Elastic Kubernetes Service (EKS). Triển khai thông lượng dữ liệu vô song của Amazon Managed Streaming for Apache Kafka (MSK) và Amazon Relational Data Service (RDS). Chạy hot/hot trên nhiều Vùng sẵn sàng của AWS trong khu vực EU-WEST-2 London (LON). Thử nghiệm khứ hồi được tiến hành từ văn phòng của FINBOURNE tại Dublin (DUB) để mô phỏng kỳ vọng độ trễ đầu cuối thực tế của người dùng cuối trên các khu vực địa lý. Bộ công cụ thử nghiệm được sử dụng để phát lại dữ liệu lịch sử ở các tốc độ quy định đã được FINBOURNE tùy chỉnh xây dựng cho Bondtape. Bondtape đã tiến hành một bài kiểm tra độ trễ toàn diện trên nguyên mẫu CTP. Họ đã sử dụng một bộ công cụ thử nghiệm được thiết kế để đưa dữ liệu định giá trái phiếu lịch sử vào nền tảng với các mức thông lượng khác nhau trong các điều kiện được kiểm soát.\nMục tiêu là đánh giá cách hệ thống xử lý các cường độ tải khác nhau và liệu các điều kiện này có bất kỳ tác động có thể đo lường nào đến độ trễ xử lý hay không. Việc có sẵn các điểm chuẩn và công cụ kiểm tra này cho phép họ đo lường hiệu quả tác động và cải tiến hiệu suất từ việc phát triển nền tảng đang diễn ra.\nBằng cách phát lại một phiên giao dịch đầy đủ 10 giờ, Bondtape nhằm xác định xem hệ thống có duy trì hiệu suất nhất quán hay có biểu hiện trôi, suy giảm hoặc tích tụ độ trễ đuôi dưới khối lượng công việc kéo dài hay không.\nTận dụng cơ sở hạ tầng có thể mở rộng và các dịch vụ AWS của FINBOURNE, nhóm Bondtape đã đo lường cách hệ thống hoạt động trong các điều kiện mà các tài nguyên được phân bổ trở nên bão hòa, và tác động (hoặc không) của các sự kiện tăng và giảm quy mô.\nKhi việc kiểm tra và đánh giá nền tảng tiếp tục cùng với sự phát triển của nó, Bondtape sẽ hợp tác chặt chẽ với AWS để đảm bảo giải pháp CTP của họ thể hiện các đặc tính hiệu suất và khả năng phục hồi cần thiết trước khi ra mắt.\nPhương pháp kiểm tra hiệu suất: Mô phỏng điều kiện thị trường Bondtape đã tiến hành hai loại kiểm tra:\nĐộ nhạy tải. Bằng cách phát lại dữ liệu trái phiếu ở các tốc độ khác nhau (1x, 2x và 3x), họ đã quan sát cách việc tăng thông lượng ảnh hưởng đến độ trễ và khả năng phản hồi của CTP, ngay cả ở trạng thái ban đầu. Tính ổn định dưới tải đại diện. Bằng cách phát lại một phiên giao dịch đầy đủ 10 giờ, Bondtape muốn xác định xem hệ thống có duy trì hiệu suất nhất quán hay có biểu hiện trôi, suy giảm hoặc tích tụ độ trễ đuôi dưới khối lượng công việc kéo dài hay không. Phương pháp này cho phép Bondtape đo lường các đặc tính độ trễ của hệ thống dưới cả áp lực ngắn hạn và hoạt động kéo dài, giúp họ hiểu rõ hơn về hành vi của nó trong các kịch bản giống như sản xuất. Họ đặt \u0026lsquo;mức chịu đựng\u0026rsquo; ban đầu là 500ms. Đối với mỗi bài kiểm tra, Bondtape đã đo thời gian khứ hồi (RTT) và tính toán các chỉ số chính bao gồm độ trễ trung bình, trung vị (P50), phân vị thứ 95 (P95) và phân vị thứ 99 (P99). Họ đã kiểm tra chất lượng mỗi sự kiện được hệ thống xử lý, bao gồm tuân thủ định dạng thông báo MiFID II, xác thực loại tài sản ISIN, đủ điều kiện ToTV (Giao dịch trên sàn giao dịch) và xác thực cờ báo cáo sau giao dịch.\nCác kết quả chính Các bài kiểm tra đã tiết lộ một số thông tin quan trọng:\nHiệu suất cơ bản: Trong điều kiện bình thường (tốc độ 1x), hệ thống duy trì độ trễ trung vị thấp, thể hiện hiệu suất vững chắc cho các khối lượng công việc thông thường. Khả năng mở rộng: Tăng tốc độ phát lại lên 2x và 3x, Bondtape quan sát thấy phân phối độ trễ rộng, cho thấy độ nhạy ngày càng tăng đối với áp lực thông lượng. Tuy nhiên, hiệu suất vẫn nằm trong giới hạn chấp nhận được. Tính ổn định lâu dài: Thử nghiệm 10 giờ cho thấy phân phối độ trễ phẳng ra, với đuôi nặng hơn ngoài 60ms. Thời gian chạy dài hơn dường như tích tụ áp lực hệ thống, làm tăng độ trễ đuôi. Các kịch bản áp suất cao: Việc phát lại dữ liệu kéo dài một tuần ở tốc độ 10x cho thấy khả năng của hệ thống trong việc xử lý khối lượng lớn trong thời gian dài, chỉ với sự gia tăng nhẹ về độ trễ. Phân phối độ trễ phẳng ra với đuôi nặng hơn ngoài 100ms. Biểu đồ tần suất ![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-1-1x-Speed.png)\nHình 1 – Tốc độ 1x\nPhân phối độ trễ cho thấy một cụm chặt chẽ quanh 30–35ms, thể hiện hiệu suất ổn định dưới tải thời gian thực.\nMột đuôi khiêm tốn bắt đầu hình thành sau 40ms, nhưng độ lan tỏa tổng thể vẫn thấp.\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-2-2x-Speed.png)\nHình 2 – Tốc độ 2x\nPhân phối dịch chuyển nhẹ sang phải, với hầu hết các độ trễ nằm trong khoảng 35 – 45ms.\u0026lt;br\u0026gt; Thông lượng tăng lên tạo ra một sự lan rộng hơn nhưng vẫn có thể quản lý được.\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-3-3x-Speed.png)\nHình 3 – Tốc độ 3x\nĐộ trễ tiếp tục tăng, với phân phối rộng hơn và đuôi dày hơn sau 50ms.\nMột số giá trị ngoại lệ bắt đầu xuất hiện, cho thấy những dấu hiệu sớm của độ nhạy tải.\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-4-Full-Day-1x.png)\nHình 4 – Cả ngày (1x)\nPhân phối độ trễ phẳng ra với một đuôi nặng hơn ngoài 60ms.\nThời gian chạy dài hơn dường như tích tụ áp lực hệ thống, làm tăng độ trễ đuôi, nhưng việc thấy các giá trị cực đoan hơn được giải thích bởi kích thước mẫu lớn hơn.\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-5-Full-Week-10x.png)\nHình 5 – Cả tuần (10x)\nPhân phối độ trễ phẳng ra với một đuôi nặng hơn ngoài 100ms.\nThời gian chạy dài hơn dường như tích tụ áp lực hệ thống, làm tăng độ trễ đuôi, nhưng việc thấy các giá trị cực đoan hơn được giải thích bởi kích thước mẫu lớn hơn.\nSự gia tăng nhẹ về độ trễ trong thử nghiệm áp lực dài hạn.\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-6-CDF-Comparison.png)\nHình 6 – So sánh CDF\nCác đường cong CDF làm nổi bật rằng tốc độ bình thường và 2x đạt 95% hoàn thành dưới 50ms, trong khi các lần chạy 3x và hàng ngày cho thấy tiến độ chậm hơn.\nĐường cong kiểm tra hàng ngày phẳng ra muộn hơn, cho thấy một sự lan rộng hơn và độ trễ đuôi cao hơn.\n![alt text](https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2025/07/17/Figure-7-Violin-Plot-Comparison.png)\nHình 7 – So sánh Biểu đồ Violin\nBiểu đồ violin cho thấy phân phối ngày càng rộng hơn từ 1x đến 3x, với lần chạy hàng ngày thể hiện sự lan rộng và các giá trị ngoại lệ đáng kể nhất.\nNó xác nhận rằng phân phối độ trễ trở nên biến đổi hơn dưới điều kiện tải kéo dài hoặc tăng tốc.\nKết luận và các bước tiếp theo Nhìn chung, nguyên mẫu Bondtape đã cho thấy hiệu suất đầy hứa hẹn, với tất cả các bài kiểm tra đều nằm trong ngưỡng chịu đựng 500ms. Hệ thống đã xử lý tốt các tải bình thường và tăng vừa phải, đồng thời cũng cho thấy khả năng phục hồi trong các kịch bản áp suất cao.\nCác bài kiểm tra cũng xác định các lĩnh vực cần cải thiện tiềm năng, đặc biệt là trong việc tối ưu hóa cho các trường hợp sử dụng thông lượng cao kéo dài hoặc chạy dài.\nBằng cách tận dụng sức mạnh và tính linh hoạt của AWS và các thử nghiệm nghiêm ngặt như thế này, Bondtape tự tin vào khả năng của đội ngũ trong việc xây dựng một Băng thông Hợp nhất mạnh mẽ, hiệu suất cao cho thị trường trái phiếu với một mức giá hợp lý. Các bước tiếp theo của Bondtape sẽ bao gồm việc tinh chỉnh nguyên mẫu dựa trên các kết quả thử nghiệm này và chuẩn bị cho việc triển khai trong thế giới thực.\nHãy theo dõi để biết thêm các cập nhật khi Bondtape và AWS tiếp tục đẩy lùi các giới hạn của những gì có thể trong xử lý và phân phối dữ liệu tài chính.\nNeil Ryan Hơn 20 năm làm việc trong lĩnh vực ngân hàng và thu nhập cố định cho một số ngân hàng toàn cầu bao gồm CEO tại Wells Fargo Bank International, IKB Credit Asset Management, chi nhánh London và Naspa Dublin. Neil cũng là một PCF-2 tại công ty dịch vụ quản lý quỹ được quy định của Northern Trust tại Ireland và là chủ tịch của Nhóm chuyên gia dữ liệu thị trường của EU. Trước khi gia nhập FINBOURNE, Neil là COO tại Corlytics (một công ty RegTech của Ireland) và Quaternion Risk Management (một công ty Fintech của Ireland), nơi ông đã xây dựng quan hệ đối tác với Deloitte, FINRA, FCA, Đại học Columbia (New York), Parameta và Acadia. Trước đây là Trợ lý Tổng thư ký tại Bộ Tài chính (Ireland, 2011-2016), CRO trong Bộ và là thành viên của RCG châu Âu của FSB. Ông có bằng luật của Trinity College, Dublin và LSE và bằng MBA của London Business School.\nGeorge Beasley George chịu trách nhiệm xây dựng các hệ thống quản lý danh tính và truy cập. Anh cũng giám sát tất cả các API của chúng tôi để đảm bảo chúng nhất quán, có thể kết hợp và cho phép các nhà phát triển của khách hàng nhanh chóng mang lại giá trị cho công ty của họ. Với vai trò là Cán bộ An ninh Thông tin, George giám sát việc phát triển và áp dụng liên tục các hệ thống, quy trình và các phương pháp bảo mật tốt nhất của FINBOURNE. Là người đồng sáng lập, George cũng là một người ủng hộ và là động lực chính của văn hóa cởi mở, minh bạch và hợp tác của chúng tôi và dẫn dắt một số sáng kiến nội bộ xây dựng và nuôi dưỡng văn hóa này. Trước khi đồng sáng lập FINBOURNE, George đã có sáu năm làm việc về công nghệ nền tảng có khả năng mở rộng và mạnh mẽ cao tại UBS Delta và ba năm làm việc trong lĩnh vực kỹ thuật phần mềm tại một công ty khởi nghiệp viễn thông.\nVivian Lai Vivian là Chuyên gia Thị trường Vốn tại AWS. Trước khi gia nhập AWS, cô đã có hơn 20 năm làm việc ở phía mua, bao gồm tại Bridgewater Associates, Amaranth Advisors và Pequot Capital, trong các vai trò công nghệ văn phòng trước. Vivian cũng là người đồng sáng lập tại Cerulean Analytics, được Trepp mua lại vào năm 2017, và là COO tại Adroit Trading Technologies. Cô nhận bằng Cử nhân Khoa học Máy tính và Cử nhân Quản lý tại Viện Bách khoa Rensselaer.\n"
},
{
	"uri": "/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section contains AWS technical blogs that have been translated from English to Vietnamese as part of the AWS First Cloud Journey internship program.\nBlog 1 - Building cost-effective RAG applications with Amazon Bedrock Knowledge Bases and Amazon S3 Vectors This blog explores how to build cost-effective Retrieval-Augmented Generation (RAG) applications using Amazon Bedrock Knowledge Bases integrated with Amazon S3 Vectors. Learn how to reduce vector storage costs by up to 90% while maintaining sub-second query performance. The article covers the complete setup process including creating knowledge bases, configuring data sources, implementing vector storage, and testing retrieval capabilities.\nBlog 2 - Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI This blog introduces Amazon Nova LLM-as-a-Judge, a comprehensive approach to evaluating generative AI model performance on Amazon SageMaker AI. Learn how to conduct systematic model quality assessments using pairwise comparisons, understand evaluation metrics including win rates and confidence intervals, and implement automated evaluation workflows. The article covers training methodology, bias validation, and practical implementation using SageMaker training jobs with detailed code examples.\nBlog 3 - Optimizing Fixed Income Consolidated Tape Performance: Latency Testing for Bond Market Data This blog examines Bondtape\u0026rsquo;s approach to building a high-performance Fixed Income Consolidated Tape Provider (CTP) solution on AWS. Discover how to architect low-latency financial data systems using Amazon EC2, Amazon EKS, Amazon MSK, and Amazon RDS across multiple Availability Zones. The article details comprehensive latency testing methodologies, performance benchmarking under various load conditions, and strategies for maintaining consistent performance in real-world trading scenarios.\n"
},
{
	"uri": "/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Explore and understand VPC (Virtual Private Cloud) fundamentals Learn EC2 service basics and deployment practices Deploy a static website using S3 and EC2 Practice AWS architecture diagramming with draw.io Understand cost optimization strategies in AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Explore VPC concepts and architecture - Practice opening and configuring VPC instances - Learn AWS architecture diagramming using draw.io 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ 2 - Attempt to develop static website application using Kiro AI - Troubleshoot implementation issues - Learn about clear requirement specification for AI-assisted development 23/09/2025 23/09/2025 3 - Practice VPC integration with S3 - Deploy simple static website using S3 - Configure S3 bucket for static website hosting 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Configure EC2 instance to host server - Explore EC2 service functionality and features - Discover AWS cost optimization strategies - Demo EC2 service capabilities 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Revision of AWS Cost Management principles - Review S3, VPC, and EC2 modules - Test static website deployment and functionality 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice AWS architecture diagrams with draw.io - Watch AWS Study Group course videos on YouTube - Consolidate week\u0026rsquo;s learnings 27/09/2025 27/09/2025 AWS Study Group YouTube Channel Week 3 Achievements: VPC Fundamentals:\nUnderstood VPC architecture and core concepts Successfully configured VPC instances Learned VPC networking and security group configurations Practiced VPC integration with other AWS services EC2 Service Mastery:\nGained hands-on experience with EC2 instance deployment Explored EC2 instance types and configurations Learned EC2 connection methods and management Understood EC2 integration with VPC Static Website Deployment:\nSuccessfully deployed a static website using S3 Configured S3 bucket for website hosting Integrated S3 with EC2 for server hosting Tested website functionality and accessibility AWS Architecture Diagramming:\nLearned to use draw.io for AWS architecture diagrams Practiced creating professional AWS infrastructure diagrams Documented VPC and EC2 configurations visually Cost Optimization:\nDiscovered AWS cost management strategies Learned methods to reduce AWS service costs Understood billing and cost monitoring tools Version Control and AI Tools:\nExplored Git version control practices Gained experience with Kiro AI capabilities and limitations Learned the importance of clear requirements for AI-assisted development Challenges Faced: Kiro AI Implementation Issues:\nEncountered significant code corruption and hallucination problems when using Kiro AI Spent 2-3 days debugging corrupted code Root cause: Unclear and insufficiently detailed requirements allowed AI to make incorrect assumptions Lesson learned: AI tools require precise, well-defined requirements to produce reliable code Decision made: Deleted corrupted repository and restarted with clearer approach Requirement Specification:\nLearned that detailed requirements are critical for successful AI-assisted development Understood the gap between general instructions and specific implementation needs Improved ability to write clear, actionable requirements Key Learnings: VPC is fundamental to AWS networking and security architecture EC2 provides flexible compute capacity with various instance types and configurations S3 can effectively host static websites with proper configuration Integration between VPC, EC2, and S3 enables robust application architectures Cost optimization is crucial for sustainable AWS usage Clear, detailed requirements are essential for successful AI-assisted development Version control practices help manage code changes and experimentation Next Week Goals: Continue exploring AWS services and their integrations Apply lessons learned about requirement specification Practice more complex AWS architecture patterns Improve cost optimization strategies "
},
{
	"uri": "/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Workshop: Data Science on AWS Location: FPT University HCM Campus\nDate: Thursday, October 16, 2025\nEvent Objectives Explore the entire journey of building a modern Data Science system, from theory to practice Master the end-to-end Data Science workflow on AWS, including storage, processing, and model deployment Gain practical experience with real datasets (IMDb) and applied models (Sentiment Analysis) Analyze trade-offs between Cloud and On-premise infrastructure in terms of cost and performance Speaker List Văn Hoàng Kha Cloud Solutions Architect, AWS Community Builder Bạch Doãn Vương Cloud DevOps Engineer, AWS Community Builder Key Content Highlights 1. Role of Cloud in Data Science and Workflow Overview Importance of Cloud: Analyze why modern data science relies on cloud platforms to achieve scalability and high integration, rather than being limited by traditional on-premise infrastructure Data Science Workflow on AWS: Storage: Use Amazon S3 as the core data lake platform ETL/Processing: Use AWS Glue for serverless data integration tasks Modeling: Leverage Amazon SageMaker as the central hub for building, training, and deploying models Overview of AWS\u0026rsquo;s extensive AI/ML ecosystem, including AI services, ML services, and infrastructure 2. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Scenario: Process and clean raw data from the IMDb dataset Technique: Demonstrate effective feature engineering and data preparation methods. The workshop highlights different approaches, from low-code options like SageMaker Canvas to code-first methods using Numpy/Pandas Demo 2: Sentiment Analysis with SageMaker: Scenario: Train and deploy a machine learning model for sentiment analysis from text data Process: Illustrate the \u0026ldquo;Train, Tune, Deploy\u0026rdquo; cycle in SageMaker Studio. The session also covers the \u0026ldquo;Bring Your Own Model (BYOM)\u0026rdquo; concept, showcasing flexibility with frameworks like TensorFlow and PyTorch 3. Strategic Discussion Cloud versus On-Premise: In-depth discussion on cost optimization and performance metrics. Content highlights how cloud elasticity enables testing heavy workloads without large upfront hardware investment for on-premise infrastructure Small Project Guide: Introduction to a project designed after the workshop to reinforce learned skills What I Learned Technical Workflow Unified Workflow: A powerful data science workflow extends beyond just code; it requires seamless integration between storage (S3), cleaning (Glue), and modeling (SageMaker) Tool Selection: Understanding when to use pre-managed services (like Amazon Comprehend or Textract) and when to build custom models on SageMaker is key to achieving efficiency Industry Application Real-world Context: The transition from academic theory to industrial application lies in automation and scalability capabilities Cost Awareness: Successful data projects must balance model accuracy with associated computational costs Application to Work Apply AWS Glue: Propose converting local ETL scripts to AWS Glue for serverless data processing automation on larger datasets Deploy SageMaker: Migrate experimental models from local Jupyter notebooks to SageMaker Studio to standardize training and deployment workflows Implement Project: Execute the small project proposed after the workshop to solidify understanding of IMDb data processing workflow Event Experience The workshop \u0026ldquo;Data Science on AWS\u0026rdquo; served as an important bridge between academic knowledge and business practice.\nDirect Connection: The event linked theoretical knowledge with technologies used by leading global enterprises Practical Perspective: Observing the process of cleaning the IMDb dataset and deploying a Sentiment Analysis model directly on the cloud simplified the complexity of AI on Cloud platforms Expert Guidance: Interacting with AWS Community Builders provided deep insights into the \u0026ldquo;Cloud versus On-premise\u0026rdquo; debate, helping me grasp the strategic value of cloud migration beyond technical features Some photos from the event Add your event photos here Overall, the workshop provided a comprehensive Data Science framework, emphasizing the importance of AWS managed tools for achieving flexibility, scalability, and cost optimization.\n"
},
{
	"uri": "/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives Deepen understanding of AWS VPC and networking concepts Learn about Amazon S3 storage service and best practices Explore AWS database services (RDS) Continue hands-on practice with core AWS services Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (30/9) - Review previous week\u0026rsquo;s EC2 concepts\n+ Instance types and sizing\n+ Security groups configuration\n- Study AWS documentation on VPC basics 30/09/2025 30/09/2025 AWS VPC Documentation Tuesday (1/10) - Learn VPC fundamentals\n+ Subnets (public vs private)\n+ Route tables\n+ Internet Gateway\n+ NAT Gateway 01/10/2025 01/10/2025 AWS VPC User Guide Wednesday (2/10) - Study Amazon S3 service\n+ Bucket creation and management\n+ Object storage concepts\n+ S3 storage classes\n+ Access control and permissions 02/10/2025 02/10/2025 AWS S3 Documentation Thursday (3/10) - Continue S3 deep dive\n+ Versioning and lifecycle policies\n+ S3 encryption options\n+ Static website hosting 03/10/2025 03/10/2025 AWS S3 Best Practices Friday (4/10) - Introduction to AWS RDS\n+ Database engine options\n+ RDS instance types\n+ Backup and restore concepts\n- Review and consolidate week\u0026rsquo;s learning 04/10/2025 04/10/2025 AWS RDS Documentation Week 4 Achievements VPC Networking Mastery\nUnderstood VPC architecture and components Learned the difference between public and private subnets Grasped routing concepts with route tables Understood how Internet Gateway and NAT Gateway work Amazon S3 Proficiency\nLearned S3 storage concepts and use cases Understood different S3 storage classes (Standard, IA, Glacier) Practiced bucket creation and object management Configured bucket policies and access controls Explored S3 versioning and lifecycle management Learned about S3 encryption options (SSE-S3, SSE-KMS) Database Services Introduction\nGained overview of AWS RDS service Understood different database engine options (MySQL, PostgreSQL, etc.) Learned about RDS instance sizing and configuration Explored backup and restore capabilities Practical Skills Development\nImproved understanding of AWS networking architecture Enhanced ability to navigate AWS documentation effectively Prepared for hands-on labs in upcoming week Challenges Faced VPC routing concepts initially seemed complex, especially understanding the relationship between route tables, subnets, and gateways S3 bucket policies and IAM permissions required careful attention to avoid access issues Balancing theoretical learning with practical hands-on time Key Learnings VPC is fundamental to AWS networking and security architecture S3 is incredibly versatile beyond simple storage - can host static websites, serve as data lake, etc. Proper subnet design (public vs private) is crucial for security best practices AWS documentation is comprehensive but requires time to digest thoroughly Next Week Goals Continue with AWS database services (RDS hands-on) Start blog translation work (first AWS technical blog) Explore AWS Lambda and serverless concepts Deepen understanding of IAM roles and policies "
},
{
	"uri": "/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AWS Mastery #1: AI/ML \u0026amp; GenAI Workshop Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Saturday, November 15, 2025\nLearning Report: \u0026ldquo;AWS AI/ML \u0026amp; GenAI Workshop\u0026rdquo; Event Objectives Provide a comprehensive overview of Artificial Intelligence/Machine Learning (AI/ML) market trends and landscape in Vietnam Provide practical guidance for building complete end-to-end ML models on Amazon SageMaker platform Deep dive into Generative AI with Amazon Bedrock, including Foundation Models, Agents, and Guardrails Equip necessary skills for Prompt Engineering and building RAG (Retrieval-Augmented Generation) applications Speaker List AWS Expert Team Key Content Highlights Welcome \u0026amp; Overview Market Overview: Update on the comprehensive landscape of artificial intelligence and machine learning (AI/ML) in the Vietnamese market context Networking: Conduct ice-breaker activities and networking to create a comfortable atmosphere for the workshop AWS AI/ML Service Overview (SageMaker) Comprehensive ML Platform: Learn the standard workflow in Amazon SageMaker, from data preparation, labeling to training and model tuning MLOps Integration: Guide on integrating machine learning operations (MLOps) to automate model deployment and management processes SageMaker Studio Demo: Directly experience the interface and functions of SageMaker Studio through practical demonstrations Generative AI with Amazon Bedrock Foundation Models Selection: Compare and provide guidance for selecting appropriate foundation models like Claude, Llama, Titan based on specific requirements Prompt Engineering Techniques: Methods for optimizing input commands, including Chain-of-Thought and Few-shot learning RAG Architecture: Analyze \u0026ldquo;Retrieval-Augmented Generation\u0026rdquo; architecture and how to integrate enterprise Knowledge Base to enhance accuracy and authenticity of AI responses Advanced Features: Guide on using Bedrock Agents for complex, multi-step workflows and Guardrails to establish safeguards ensuring safe and appropriate content Live Demo: Build a complete GenAI Chatbot at the workshop using Amazon Bedrock What I Learned Platform Capabilities SageMaker is a powerful tool designed to standardize and efficiently manage workflows for traditional Machine Learning tasks (prediction) Bedrock provides the fastest approach to GenAI through API, eliminating the need for managing complex infrastructure Strategic Implementation RAG and Agents are two cutting-edge technologies helping transform GenAI applications from simple chat functions to solving complex business problems Guardrails are essential components ensuring AI operates within safe boundaries, complying with company rules and policies Application to Work Deploy MLOps: Apply standardized processes learned on SageMaker to manage the lifecycle of machine learning models in current projects Build RAG: Experiment with integrating internal documents into Bedrock Knowledge Base to develop specialized information retrieval assistants Optimize Prompts: Apply Chain-of-Thought techniques and other methods to significantly improve quality and depth of responses from existing chatbots Evaluate Models: Use provided criteria to select the optimal foundation model (Claude vs. Llama) for cost and performance on each specific use case Event Experience The workshop was a balanced and efficient combination of traditional Machine Learning and modern Generative AI, providing a solid and comprehensive foundation of knowledge.\nPractice \u0026amp; Demonstrations The SageMaker Studio walkthrough helped me clearly visualize a professional, standardized working environment for Data Scientists Demo building Chatbot with Bedrock was an important highlight, demonstrating that creating a complete GenAI application has become faster and more accessible than ever Market Information The introduction to the AI market in Vietnam helped me position my understanding and grasp the potential opportunities of enterprises in the broader technology trends Some photos from the event Add your event photos here In summary, this event equipped me with a comprehensive \u0026ldquo;toolkit\u0026rdquo;: from SageMaker for predictive models to Bedrock for generative models, ready for upcoming large-scale AI project deployments.\n"
},
{
	"uri": "/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "Throughout the internship period, I participated in 7 significant events, each providing valuable learning experiences, new knowledge, and memorable moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: GenAI and Data Track\nDate \u0026amp; Time: Friday, September 26, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Workshop covering Agentic AI, Unified Data Foundation, GenAI roadmap, AI-Driven SDLC, Security, and AI Agents for business applications.\nEvent 2 Event Name: AI-Driven Development Life Cycle\nDate \u0026amp; Time: Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Practical workshop on AI-driven SDLC paradigm shift, Amazon Q Developer demonstrations, and Kiro tool capabilities for development optimization.\nEvent 3 Event Name: Workshop: Data Science on AWS\nDate \u0026amp; Time: Thursday, October 16, 2025\nLocation: FPT University HCM Campus\nRole: Participant\nBrief Description: End-to-end Data Science workflow on AWS, including data processing with AWS Glue, sentiment analysis with SageMaker, and cost-performance trade-off analysis.\nEvent 4 Event Name: AWS Mastery #1: AI/ML \u0026amp; GenAI Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Comprehensive overview of AI/ML market trends, SageMaker platform for ML models, Generative AI with Amazon Bedrock, Prompt Engineering, and RAG architecture.\nEvent 5 Event Name: AWS Mastery 2: AWS DevOps \u0026amp; Modern Operations\nDate \u0026amp; Time: Monday, November 17, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: DevOps mindset development, complete CI/CD workflow with AWS Developer Tools, Infrastructure as Code with CloudFormation and CDK, and comprehensive observability setup.\nEvent 6 Event Name: AWS Security Specialty Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Deep dive into 5 Core Security Pillars (IAM, Detection, Infrastructure, Data Protection, Incident Response), Zero Trust mindset, and automation-first security strategies.\nEvent 7 Event Name: CloudThinker: Agentic AI \u0026amp; Orchestration on AWS\nDate \u0026amp; Time: Friday, December 5, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Advanced technical workshop on AWS Bedrock Agent Core, Agentic Workflows, multi-agent orchestration, context optimization, and hands-on CloudThinker Hack prototype building.\nThese events provided a comprehensive learning journey from foundational cloud architecture to advanced AI/ML applications, significantly enhancing both technical expertise and professional development throughout the internship program.\n"
},
{
	"uri": "/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives Apply VPC and S3 knowledge through hands-on labs Deepen understanding of AWS RDS with practical exercises Begin AWS technical blog translation work Explore AWS Lambda and serverless computing concepts Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (6/10) Office Workday\n- Hands-on lab: Create custom VPC\n+ Configure public and private subnets\n+ Set up route tables and Internet Gateway\n- Practice S3 bucket operations\n+ Upload and manage objects\n+ Configure bucket policies and permissions 06/10/2025 06/10/2025 Lab exercises Tuesday (7/10) - Review VPC lab results and document learnings\n- Study AWS RDS hands-on tutorials\n+ Database instance creation\n+ Connection configuration\n+ Security group setup 07/10/2025 07/10/2025 AWS RDS Getting Started Wednesday (8/10) Office Workday\n- Hands-on lab: Create RDS instance\n+ Launch MySQL database\n+ Configure security groups\n+ Test database connectivity\n- Introduction to AWS Lambda\n+ Serverless computing concepts\n+ Lambda function basics 08/10/2025 08/10/2025 AWS Lambda Documentation Thursday (9/10) - Continue Lambda exploration\n+ Function triggers and events\n+ Lambda execution environment\n+ IAM roles for Lambda\n- Review serverless architecture patterns 09/10/2025 09/10/2025 AWS Serverless Friday (10/10) - Begin first AWS technical blog translation\n+ Select appropriate blog post\n+ Start translation work\n+ Research technical terminology\n- Review week\u0026rsquo;s achievements and plan next week 10/10/2025 10/10/2025 AWS Blog posts Week 5 Achievements Hands-on VPC Implementation\nSuccessfully created custom VPC with proper architecture Configured public and private subnets correctly Set up route tables and Internet Gateway Validated network connectivity and routing Gained practical experience with VPC components S3 Practical Skills\nCreated and configured S3 buckets Uploaded and managed objects successfully Implemented bucket policies for access control Practiced S3 operations through AWS Console Understood S3 security best practices RDS Database Experience\nLaunched first RDS MySQL instance Configured database security groups properly Established database connectivity Understood RDS backup and maintenance concepts Learned database instance management Serverless Computing Introduction\nGrasped Lambda and serverless concepts Understood function triggers and events Learned about Lambda execution environment Explored IAM roles for Lambda functions Reviewed serverless architecture patterns Blog Translation Initiative\nStarted first AWS technical blog translation Researched AWS technical terminology in Vietnamese Developed translation workflow Enhanced technical writing skills Challenges Faced VPC lab required careful attention to subnet CIDR blocks and routing configuration RDS security group configuration needed multiple attempts to get connectivity working Translating technical AWS terminology while maintaining accuracy and clarity Balancing hands-on lab time with theoretical learning Key Learnings Hands-on practice is essential for solidifying theoretical VPC knowledge Proper security group configuration is critical for RDS connectivity Serverless computing represents a paradigm shift from traditional infrastructure Technical translation requires deep understanding of both source and target concepts AWS services integrate seamlessly when properly configured Next Week Goals Continue blog translation work Explore more Lambda functions and triggers Learn about API Gateway and serverless APIs Study CloudWatch for monitoring and logging Deepen IAM knowledge with advanced policies "
},
{
	"uri": "/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "AWS Mastery 2: AWS DevOps \u0026amp; Modern Operations Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Monday, November 17, 2025\nLearning Report: \u0026ldquo;AWS DevOps \u0026amp; Modern Operations\u0026rdquo; Event Objectives Build DevOps mindset and master performance measurement through DORA metrics Establish complete CI/CD workflow, automating with AWS Developer Tools suite Modernize infrastructure management through code (Infrastructure as Code - IaC) with CloudFormation and CDK Deploy containerized applications (Docker) on services like ECS, EKS and App Runner Build comprehensive Observability systems for distributed applications Speaker List AWS Expert Team (Deep specialists in DevOps, Container and Observability) Key Content Highlights DevOps Mindset \u0026amp; CI/CD DORA Metrics: Emphasize importance of performance metrics like Deployment Frequency, Lead Time for Changes, and Mean Time to Recover (MTTR) Git Strategy: Compare source code management strategies like GitFlow and Trunk-based development Pipeline Automation: Illustrate complete CI/CD workflow: from CodeCommit (code repository), CodeBuild (build/test), to CodeDeploy (deployment) orchestrated by CodePipeline Deployment Strategies: Introduce safe deployment techniques to minimize risk: Blue/Green, Canary, and Rolling updates Infrastructure as Code (IaC) CloudFormation: Guide managing infrastructure through templates, including Stacks concept and drift detection mechanisms AWS CDK: Use popular programming languages to define infrastructure, leveraging reusable \u0026ldquo;Constructs\u0026rdquo; and design patterns IaC Selection: Discuss criteria for choosing between CloudFormation and CDK depending on project requirements and scale Container Services Spectrum of Compute: From image management with ECR to orchestration options: ECS (simple, deep AWS integration), EKS (open Kubernetes standard), and App Runner (maximum simplification for web apps/API) Microservices Deployment: Compare and demonstrate deploying microservices across different platforms Monitoring \u0026amp; Observability Comprehensive Observability: Combine CloudWatch (for Metrics, Logs, Alarms) and X-Ray (for Distributed Tracing) for deep and comprehensive system visibility Best Practices: Set up visual monitoring dashboards and efficient on-call processes What I Learned Automation First Priority CI/CD is a cultural foundation that minimizes human error and accelerates product release velocity IaC is a mandatory standard for all modern infrastructure, ensuring Dev/Test/Prod environments remain consistent and reproducible Operational Excellence Observability is more critical than simple Monitoring, especially in Microservices architecture, as it enables rapid root cause analysis through tracing Choosing appropriate deployment strategies (like Blue/Green) is key to achieving near-zero downtime Application to Work Restructure Pipeline: Convert current manual build processes to AWS CodePipeline integrating automated testing steps Apply CDK: Start using AWS CDK to define infrastructure for new projects, moving away from direct Console operations Containerize: Package applications in Docker and experiment deploying them to AWS App Runner for smaller services Set Up Tracing: Integrate AWS X-Ray into applications to monitor latency and communication flows between microservices Event Experience The event systematically organized knowledge seamlessly, progressing from mindset formation to tool introduction and operational procedures.\nIntegration Process The \u0026ldquo;Full CI/CD pipeline walkthrough\u0026rdquo; demo was impressive, helping me clearly visualize the entire journey of source code from developer machine to Production environment I clearly understood the differences and specific use cases of ECS versus EKS, increasing confidence when proposing containerization solutions to the company High Practicality Lessons about Deployment strategies (like Feature flags, Canary) have high real-world application value, addressing the team\u0026rsquo;s \u0026ldquo;deployment anxiety\u0026rdquo; The career roadmap guidance at the end provided clear DevOps skill development roadmap Some photos from the event Add your event photos here In summary, the workshop clarified the tight and inseparable connection between Code, Infrastructure as Code, and Observability in modern operations.\n"
},
{
	"uri": "/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives Attend Data Science on AWS Workshop at FPT University Continue AWS blog translation work Explore AWS data analytics services Learn about API Gateway and serverless APIs Deepen CloudWatch monitoring knowledge Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (13/10) - Continue blog translation work\n+ Translate technical sections\n+ Review terminology accuracy\n- Study API Gateway documentation\n+ REST API concepts\n+ API Gateway integration with Lambda 13/10/2025 13/10/2025 AWS API Gateway Tuesday (14/10) - Learn about CloudWatch monitoring\n+ Metrics and alarms\n+ Log groups and streams\n+ CloudWatch dashboards\n- Review serverless API patterns 14/10/2025 14/10/2025 AWS CloudWatch Wednesday (15/10) - Prepare for Data Science workshop\n+ Review data science concepts\n+ Study AWS data services overview\n- Continue blog translation progress 15/10/2025 15/10/2025 Workshop materials Thursday (16/10) Data Science on AWS Workshop - FPT University\nTime: 9:30 AM - 11:45 AM\nSpeakers: Văn Hoàng Kha \u0026amp; Bạch Doãn Vương (AWS Community Builders)\nWorkshop Agenda:\n- Introduction \u0026amp; Cloud importance in Data Science\n- Data Science pipeline on AWS (S3, Glue, SageMaker)\n- Demo 1: Data processing with AWS Glue (IMDb dataset)\n- Demo 2: Sentiment Analysis with SageMaker\n- Discussion: Cloud vs On-premise cost \u0026amp; performance\n- Post-workshop project guidance 16/10/2025 16/10/2025 Workshop at FPT University Friday (17/10) - Review workshop learnings and notes\n- Research AWS Glue and SageMaker further\n+ ETL job concepts\n+ SageMaker notebook instances\n- Complete blog translation draft 17/10/2025 17/10/2025 Workshop notes Week 6 Achievements Data Science on AWS Workshop\nAttended comprehensive workshop at FPT University Learned about Data Science pipeline on AWS Understood AWS Glue for data processing and ETL Explored Amazon SageMaker for ML model training Witnessed live demos with real datasets (IMDb) Gained insights on cloud vs on-premise comparison Networked with AWS Community Builders Received guidance for post-workshop projects AWS Data Services Knowledge\nUnderstood S3 role in data lakes Learned AWS Glue for ETL operations Explored SageMaker capabilities for ML workflows Grasped data processing pipeline architecture Understood cost and performance considerations API Gateway and Serverless APIs\nLearned REST API concepts with API Gateway Understood API Gateway integration with Lambda Explored API deployment and stages Studied API authentication and authorization Reviewed serverless API architecture patterns CloudWatch Monitoring\nLearned about metrics and alarms Understood log groups and streams Explored CloudWatch dashboards Studied monitoring best practices Understood observability concepts Blog Translation Progress\nMade significant progress on first blog translation Refined technical terminology translations Completed draft of blog translation Enhanced technical writing skills in Vietnamese Challenges Faced Workshop covered advanced topics quickly - required focused attention Understanding ETL concepts and AWS Glue job configuration Balancing workshop learning with ongoing blog translation work Translating complex data science terminology accurately Key Learnings AWS provides comprehensive data science and ML platform AWS Glue simplifies ETL operations significantly SageMaker enables end-to-end ML workflows Cloud offers flexibility and scalability for data science workloads Hands-on demos are invaluable for understanding complex services Community builders provide practical real-world insights Next Week Goals Finalize and publish first blog translation Practice AWS Glue with sample datasets Explore SageMaker notebook instances Continue with Lambda and API Gateway hands-on Review and consolidate data analytics knowledge "
},
{
	"uri": "/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship with the AWS First Cloud Journey Program from September 2025 to December 2025, I had the opportunity to learn, practice, and apply cloud computing knowledge in a structured learning environment focused on AWS services and technologies.\nI participated in comprehensive AWS cloud learning activities, including hands-on labs with core AWS services (EC2, S3, VPC, RDS, Lambda), attended multiple AWS events and workshops, translated technical AWS blogs from English to Vietnamese, and collaborated on a team project implementing cloud solutions. Through these activities, I improved my skills in cloud architecture, technical documentation, translation, problem-solving, and teamwork.\nIn terms of work ethic, I consistently completed weekly learning objectives, maintained detailed worklogs, actively participated in AWS community events, and collaborated effectively with team members on the final project.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "/4-eventparticipated/4.6-event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "AWS Security Specialty Workshop Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Saturday, November 29, 2025\nLearning Report: \u0026ldquo;AWS Security Specialty Workshop\u0026rdquo; Event Objectives Deeply understand the role of Security Pillar in AWS Well-Architected Framework Master the 5 Core Security Pillars: Identity and Access Management (IAM), Detection, Infrastructure Security, Data Protection, and Incident Response Update on leading cybersecurity threats in Cloud environments in the Vietnamese market Practice skills like IAM privilege reviews and building incident response playbooks (IR Playbook) Speaker List AWS Security Expert Team (Deep expertise in security architecture and compliance) Key Content Highlights Foundation \u0026amp; Identity (Pillar 1) Core Principles: Strictly apply Least Privilege, Zero Trust (default deny), and Defense in Depth principles Modern IAM: Transition from IAM Users (with long-term credentials) to IAM Roles and AWS Identity Center (SSO) for centralized access management Access Control: Use Service Control Policies (SCP) and Permission Boundaries to limit privilege scope in multi-account environments Small Practical Demo: Demonstrate how to validate IAM Policies and simulate access to detect security configuration errors Detection \u0026amp; Infrastructure (Pillar 2 \u0026amp; 3) Continuous Monitoring: Enable services like CloudTrail (at organization level), GuardDuty, and Security Hub for continuous security monitoring and assessment Logging Strategy: Require logging at every system layer: VPC Flow Logs (network), ALB logs (application), and S3 logs (storage) Network Security: Implement network segmentation with VPC, combining Security Groups and NACLs. Protect perimeter with WAF, Shield, and Network Firewall Data Protection \u0026amp; Incident Response (Pillar 4 \u0026amp; 5) Encryption: Implement data encryption in-transit and at-rest on services like S3, EBS, RDS, using KMS (Key Management Service) Secrets Management: Eliminate hard-coded credentials by using Secrets Manager and Parameter Store, combining automatic rotation mechanisms Incident Response Automation: Build response playbooks for common incidents (like exposed access keys, malware) and automate resource isolation using Lambda/Step Functions What I Learned Zero Trust Mindset Identity as the Perimeter: In Cloud environments, Identity has become the most important protective barrier, replacing traditional IP addresses Always follow the principle: Never trust by default, always authenticate every request, and grant only minimum necessary permissions Automation is Core Manual security cannot keep pace with Cloud deployment speed. Must apply methods like Detection-as-Code and Auto-remediation to minimize risk from human error and delays Application to Work Audit IAM: Review all IAM Users, delete old access keys, and migrate applications to IAM Roles for enhanced security Enable GuardDuty: Activate GuardDuty service across all regions and accounts to detect abnormal access and activities early Deploy Secrets Manager: Replace configuration files containing database passwords with API calls to Secrets Manager Build IR Playbook: Write detailed incident handling procedures for \u0026ldquo;IAM Key Exposure\u0026rdquo; scenario and conduct drills with technical team Event Experience The workshop provided deep technical details, comprehensively covering essential security aspects modern Cloud engineers must master.\nComprehensive Framework Organizing content by 5 Security Pillars helped me systematize previously scattered security knowledge into a standard, applicable framework The discussion on Leading threats in Vietnam is highly practical, helping the team identify specific risks suited to local context Applied Practice Demos on Access Analyzer and Validate IAM Policy tools are very useful, directly addressing daily permission debugging challenges The Incident Response section clarified that \u0026ldquo;detection\u0026rdquo; is only part of the solution; fast automatic \u0026ldquo;response\u0026rdquo; is what determines system safety Some photos from the event Add your event photos here Overall, the event clearly confirmed that security is not a blocker but an enabler allowing enterprises to operate faster and safer.\n"
},
{
	"uri": "/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives Finalize and publish first AWS blog translation Explore AWS container services (ECS, ECR) Learn about AWS security best practices Review and consolidate previous weeks\u0026rsquo; learning Begin second blog translation Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (20/10) - Finalize first blog translation\n+ Final review and editing\n+ Verify technical accuracy\n+ Format for publication\n- Publish first blog translation 20/10/2025 20/10/2025 Blog translation work Tuesday (21/10) - Introduction to AWS container services\n+ Docker fundamentals review\n+ Amazon ECS overview\n+ Amazon ECR (Elastic Container Registry)\n- Study container orchestration concepts 21/10/2025 21/10/2025 AWS ECS Documentation Wednesday (22/10) - Continue ECS deep dive\n+ ECS task definitions\n+ ECS services and clusters\n+ Fargate vs EC2 launch types\n- Review container networking 22/10/2025 22/10/2025 AWS Fargate Thursday (23/10) - AWS security best practices\n+ IAM policies and roles review\n+ Security groups best practices\n+ Encryption at rest and in transit\n+ AWS security services overview 23/10/2025 23/10/2025 AWS Security Best Practices Friday (24/10) - Review and consolidation\n+ Revisit VPC concepts\n+ Review serverless architecture\n+ Consolidate data services knowledge\n- Select and begin second blog translation 24/10/2025 24/10/2025 Previous weeks\u0026rsquo; materials Week 7 Achievements Blog Translation Milestone\nSuccessfully completed first AWS blog translation Published translated blog post Established translation workflow and quality standards Enhanced technical writing skills in both languages Started second blog translation Container Services Knowledge\nUnderstood Docker fundamentals and containerization Learned Amazon ECS architecture and components Explored Amazon ECR for container image management Understood ECS task definitions and services Compared Fargate vs EC2 launch types Grasped container orchestration concepts Security Best Practices\nReviewed IAM policies and roles in depth Understood security group configuration best practices Learned about encryption options (at rest and in transit) Explored AWS security services (GuardDuty, Security Hub, etc.) Understood principle of least privilege Learned about AWS shared responsibility model Knowledge Consolidation\nReviewed VPC networking concepts Consolidated serverless architecture understanding Reinforced data services knowledge Connected concepts across different AWS services Identified areas for deeper study Challenges Faced Container orchestration concepts required careful study Understanding the differences between ECS and EKS Balancing new learning with review and consolidation Managing time between blog translation and technical learning Key Learnings Containers provide consistency across development and production environments ECS simplifies container orchestration on AWS Fargate removes infrastructure management burden Security should be considered at every layer of architecture Regular review and consolidation strengthens understanding Translation work deepens technical comprehension Next Week Goals Continue second blog translation Explore AWS EKS (Kubernetes on AWS) Learn about AWS Well-Architected Framework Study AWS cost optimization strategies Prepare for upcoming midterm exam period "
},
{
	"uri": "/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Learning Environment\nThe AWS First Cloud Journey program provides an excellent self-paced learning environment with comprehensive resources. The combination of hands-on labs, documentation, and real AWS services creates an authentic cloud learning experience. The program structure allows flexibility while maintaining clear learning objectives and milestones.\n2. Support from Mentors / Program Team\nThe FCJ team and mentors provide excellent guidance through well-organized events and workshops. The AWS Cloud Mastery Series sessions were particularly valuable, offering deep dives into AI/ML, DevOps, and Security topics. The speakers from AWS and industry partners shared practical insights that complemented the self-study materials effectively.\n3. Relevance to Academic Major \u0026amp; Career Goals\nThe program content aligns perfectly with modern cloud computing curriculum and industry demands. Learning AWS services hands-on, combined with technical blog translation, provided both technical depth and communication skills essential for cloud engineering roles. The team project component added valuable collaborative experience.\n4. Learning \u0026amp; Skill Development Opportunities\nThe program offered diverse learning opportunities: technical skills through AWS service exploration, documentation skills through worklog maintenance, translation skills through blog projects, and teamwork through the final project. Attending multiple AWS events (Cloud Day, Data Science Workshop, Cloud Mastery Series) provided exposure to current industry trends and networking opportunities.\n5. Community \u0026amp; Collaboration\nThe AWS FCJ community is supportive and collaborative. Events like the Kick-off session and Cloud Mastery Series created opportunities to connect with fellow learners, AWS professionals, and industry experts. The team project phase fostered strong collaboration and knowledge sharing among participants.\n6. Program Structure \u0026amp; Resources\nThe program provides clear structure with weekly learning objectives, comprehensive AWS documentation access, and practical hands-on opportunities. The progression from foundational services to advanced topics (AI/ML, DevOps, Security) is well-designed. Access to AWS services for hands-on practice is invaluable.\nAdditional Questions What did you find most satisfying during your internship?\nThe most satisfying aspects were the hands-on experience with real AWS services and attending the AWS Cloud Mastery Series. Being able to deploy actual cloud infrastructure and see concepts from documentation come to life was incredibly rewarding. The networking opportunities at AWS events and connecting with industry professionals were also highlights.\nWhat do you think the program should improve for future participants?\nConsider providing more structured guidance for the team project phase, perhaps with milestone checkpoints. Additional workshops on specific AWS certifications (like Cloud Practitioner or Solutions Architect Associate) would be valuable. More opportunities for peer collaboration and knowledge sharing throughout the program would enhance the learning experience.\nIf recommending to a friend, would you suggest they participate in this program? Why or why not?\nAbsolutely yes. The AWS FCJ program offers unparalleled hands-on cloud learning experience with access to real AWS services, expert-led workshops, and a supportive community. It\u0026rsquo;s perfect for anyone serious about building a career in cloud computing. The combination of self-paced learning and structured events provides flexibility while ensuring comprehensive skill development.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the program experience?\nIntroduce a mentorship pairing system where experienced participants can guide newcomers Create a shared knowledge base or wiki where participants can document solutions to common challenges Organize more frequent virtual meetups for participants to share progress and learnings Provide sample project ideas or templates to help kickstart the team project phase Consider adding AWS certification exam preparation resources and study groups Would you like to continue this program in the future?\nYes, I would be very interested in continuing with advanced AWS learning paths, perhaps focusing on specialized areas like AI/ML on AWS, DevOps practices, or cloud security. An alumni network or advanced track for FCJ graduates would be valuable for continued learning and professional development.\nAny other comments (free sharing):\nThe AWS First Cloud Journey program exceeded my expectations in terms of learning depth and practical experience. The combination of self-study, hands-on labs, expert-led workshops, and team collaboration created a comprehensive learning journey. Special thanks to the FCJ team for organizing such high-quality events and providing continuous support throughout the program. This experience has significantly strengthened my cloud computing skills and career readiness.\n"
},
{
	"uri": "/4-eventparticipated/4.7-event7/",
	"title": "Event 7",
	"tags": [],
	"description": "",
	"content": "CloudThinker: Agentic AI \u0026amp; Orchestration on AWS Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Friday, December 5, 2025\nLearning Report: \u0026ldquo;CloudThinker: Agentic AI \u0026amp; Orchestration on AWS\u0026rdquo; Event Objectives Deep dive into AWS Bedrock Agent Core and its core capabilities Explore real-world use cases for building Agentic Workflows Master advanced concepts like Agentic Orchestration and Context Optimization at deep technical level (L300) Gain hands-on experience through CloudThinker Hack workshop Speaker List Nguyễn Gia Hưng Head of Solutions Architect, AWS Kiên Nguyễn Solutions Architect, AWS Việt Phạm Founder \u0026amp; CEO, CloudThinker Thắng Tôn Co-founder \u0026amp; COO, CloudThinker Henry Bùi Head of Engineering, CloudThinker Kha Văn Workshop Facilitator Key Content Highlights 1. AWS Foundation and Agentic AI Opening: Nguyễn Gia Hưng opens the event, emphasizing the increasingly critical role of Agentic AI in today\u0026rsquo;s cloud technology landscape Bedrock Agent Core: Kiên Nguyễn provides technical overview of AWS Bedrock Agent Core, explaining how this service simplifies creating Agents capable of planning and executing tasks through API calls 2. Real-World Applications Building Agentic Workflows: Việt Phạm demonstrates a specific use case, illustrating the design and deployment of an agentic workflow from start to finish on AWS platform CloudThinker Introduction: Thắng Tôn introduces CloudThinker ecosystem and the company\u0026rsquo;s vision for AI-integrated cloud solutions 3. Deep Dive (Level 300) Agent Orchestration \u0026amp; Context Optimization: This is the deep technical section of the morning. Henry Bùi discusses advanced strategies for orchestrating multi-agent systems and context optimization in Amazon Bedrock, ensuring high accuracy in complex interactions 4. Hands-on Practice CloudThinker Hack: Facilitated by Kha Văn, this 60-minute hands-on session allows participants to directly apply learned knowledge to build an agent prototype using CloudThinker framework and AWS services Key Takeaways Evolution of AI From Chat to Action: The technology field is witnessing a strong shift from chatbots that only respond passively to proactive Agents capable of performing complex orchestration and calling APIs to complete tasks Context is Key: As workflows become increasingly complex, standard context windows are insufficient. Applying \u0026ldquo;Context Optimization\u0026rdquo; strategies is essential for reducing operational costs while increasing Agent accuracy Architecture Orchestration Model: Managing a system with multiple Agents requires a strong Orchestration Layer to allocate and decide which Agent handles which part of user requests Application to Work Prototype Agent: Use AWS Bedrock Agent to build simple internal tools capable of connecting to existing company APIs (for example: checking server status or leave statistics) Research Context Models: Deep dive into context optimization techniques shared by Henry Bùi to apply to current RAG systems for improved performance Join Hackathons: Encourage technical team to participate in similar real hackathons for the fastest and most intuitive updates on AWS new features Event Experience The event was highly technical and deeply focused on a single topic.\nTechnical Depth: The L300 session on Orchestration was particularly valuable, helping me understand how to scale AI applications beyond simple demos Interactivity: The \u0026ldquo;CloudThinker Hack\u0026rdquo; section helped me immediately reinforce theoretical knowledge with practice, making this one of the most effective learning sessions Some photos from the event Add your event photos here Overall, this event provided a clear architectural framework for developing Agentic AI Systems capable of complex orchestration and execution on AWS cloud platform.\n"
},
{
	"uri": "/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Prepare comprehensively for AWS Cloud Practitioner midterm exam Review all AWS services covered in previous weeks Consolidate understanding of AWS fundamentals Practice exam-style questions and scenarios Successfully complete midterm examination Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (27/10) Midterm Revision - Compute \u0026amp; Storage\n- Review EC2 fundamentals\n+ Instance types and pricing\n+ AMI and EBS concepts\n+ Security groups\n- Review S3 storage\n+ Storage classes\n+ Bucket policies\n+ Versioning and lifecycle 27/10/2025 27/10/2025 AWS Cloud Practitioner study materials Tuesday (28/10) Midterm Revision - Networking \u0026amp; Database\n- Review VPC concepts\n+ Subnets and routing\n+ Internet Gateway and NAT\n+ Security groups vs NACLs\n- Review RDS and database services\n+ Database engines\n+ Backup and restore\n- Registered for office workday (29/10) 28/10/2025 28/10/2025 AWS documentation Wednesday (29/10) Day Off - Heavy Rain\n- Unable to attend office due to severe weather conditions\n- Light study at home\n+ Review study notes\n+ Practice questions 29/10/2025 29/10/2025 Study notes Thursday (30/10) Office Workday - Final Revision\n- Intensive exam preparation\n- Review serverless services\n+ Lambda functions\n+ API Gateway\n- Review data analytics services\n+ AWS Glue\n+ SageMaker basics\n- Practice exam questions\n- Clarify doubts with mentors\n- Registered for midterm exam (31/10) 30/10/2025 30/10/2025 Exam preparation materials Friday (31/10) AWS Cloud Practitioner Midterm Exam\n- Completed midterm examination\n- Exam covered:\n+ AWS global infrastructure\n+ Core services (EC2, S3, VPC, RDS)\n+ Serverless computing\n+ Security and IAM\n+ Pricing and billing\n+ AWS Well-Architected Framework basics 31/10/2025 31/10/2025 Midterm exam Week 8 Achievements Comprehensive Exam Preparation\nSystematically reviewed all AWS services from previous weeks Consolidated understanding of AWS fundamentals Practiced exam-style questions and scenarios Identified and addressed knowledge gaps Prepared comprehensive study notes AWS Services Review\nCompute: EC2 instance types, pricing models, AMI, EBS Storage: S3 storage classes, bucket policies, lifecycle management Networking: VPC architecture, subnets, routing, security Database: RDS engines, backup strategies, read replicas Serverless: Lambda functions, API Gateway, event-driven architecture Data Analytics: AWS Glue ETL, SageMaker basics Security: IAM policies, roles, security groups, encryption Monitoring: CloudWatch metrics, alarms, logs Midterm Exam Completion\nSuccessfully completed AWS Cloud Practitioner midterm exam Demonstrated understanding of AWS core services Applied knowledge to practical scenarios Showed comprehension of AWS best practices Adaptability\nManaged unexpected weather disruption professionally Adjusted study schedule to accommodate circumstances Maintained focus despite challenges Challenges Faced Heavy rain on Wednesday disrupted planned office workday Time pressure to review extensive material before exam Balancing breadth vs depth in exam preparation Managing exam stress while maintaining study effectiveness Key Learnings Systematic review is essential for comprehensive exam preparation Hands-on experience significantly aids theoretical understanding AWS services are interconnected - understanding relationships is crucial Regular documentation and note-taking throughout learning pays off Adaptability is important when facing unexpected circumstances Next Week Goals Resume normal AWS learning schedule post-exam Continue second blog translation work Begin team project preparation (starting Week 9) Explore AWS DevOps services Learn about AWS cost optimization strategies "
},
{
	"uri": "/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives Transition from individual AWS study to team project work Form project team and establish collaboration workflow Define team project scope and requirements Begin project planning and architecture design Continue blog translation work Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (3/11) - Post-midterm review and reflection\n- Continue second blog translation\n+ Translate technical sections\n+ Review terminology\n- Prepare for team project kickoff 03/11/2025 03/11/2025 Blog translation work Tuesday (4/11) - Final preparations for team project\n- Review AWS services for project ideas\n- Research potential project architectures\n+ Serverless applications\n+ Web applications on AWS\n+ Data processing pipelines 04/11/2025 04/11/2025 AWS architecture patterns Wednesday (5/11) Team Project Kickoff\n- Team formation and introductions\n- Initial project brainstorming session\n- Discuss project ideas and scope\n- Identify team member strengths and interests\n- Establish communication channels\n+ Team chat group\n+ Document sharing\n+ Meeting schedule 05/11/2025 05/11/2025 Team collaboration Thursday (6/11) Team Project Planning\n- Define project scope and objectives\n- Select project architecture approach\n- Identify required AWS services\n- Create initial project timeline\n- Assign preliminary roles and responsibilities\n- Set up project repository and documentation 06/11/2025 06/11/2025 Project planning documents Friday (7/11) Project Requirements \u0026amp; Design\n- Document project requirements\n- Create architecture diagrams\n- Plan AWS resource allocation\n- Discuss implementation approach\n- Identify potential challenges\n- Plan for next week\u0026rsquo;s development tasks 07/11/2025 07/11/2025 Project documentation Week 9 Achievements Team Project Initiation\nSuccessfully formed project team Established effective team communication channels Defined clear project scope and objectives Created initial project timeline Assigned roles based on team member strengths Set up project infrastructure (repository, documentation) Project Planning\nDocumented comprehensive project requirements Designed initial system architecture Identified required AWS services for implementation Created architecture diagrams Planned resource allocation strategy Established development workflow Team Collaboration\nBuilt rapport with team members Established regular meeting schedule Created shared documentation system Defined communication protocols Fostered collaborative environment Blog Translation Progress\nContinued work on second blog translation Made significant progress on technical sections Maintained translation quality standards Challenges Faced Transitioning from individual learning to team collaboration Aligning team members\u0026rsquo; schedules and availability Balancing different skill levels and experience within team Defining realistic project scope within time constraints Coordinating multiple perspectives on project direction Key Learnings Effective team communication is foundation for successful collaboration Clear role definition prevents confusion and overlap Early planning saves time during implementation Team diversity brings valuable different perspectives Documentation is crucial for team coordination Regular check-ins keep everyone aligned Next Week Goals Begin project implementation Set up AWS infrastructure for project Develop core project components Continue regular team meetings and coordination Maintain project documentation Prepare for AWS Cloud Mastery Series #1 (15/11) "
},
{
	"uri": "/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives Continue team project development Implement core project features Attend AWS Cloud Mastery Series #1 on AI/ML/GenAI Maintain team coordination and documentation Adapt workflow after injury incident Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (10/11) Team Project Development\n- Team meeting and progress review\n- Continue project implementation\n- Develop core features\n- Code review and integration\n- Update project documentation 10/11/2025 10/11/2025 Project repository Tuesday (11/11) Office Workday - Project Sprint\n- Intensive development session at office\n- Implement key project components\n- Team collaboration and pair programming\n- Resolve technical challenges\n- Test and debug features\n- Coordinate with team members 11/11/2025 11/11/2025 Project development Wednesday (12/11) Team Project Coordination\n- Remote team meeting\n- Progress updates and status sync\n- Address blockers and issues\n- Plan remaining development tasks\n- Prepare for weekend AWS event 12/11/2025 12/11/2025 Team collaboration Thursday (13/11) Project Development \u0026amp; Testing\n- Continue feature implementation\n- Write and run tests\n- Bug fixes and improvements\n- Documentation updates\n- Code quality review 13/11/2025 13/11/2025 Development work Friday (14/11) Hand Injury Accident\n- Accident occurred during the day\n- Hospitalized and received medical treatment\n- Got cast for hand injury\n- Unable to continue normal work\n- Informed team about situation 14/11/2025 14/11/2025 Medical treatment Saturday (15/11) AWS Cloud Mastery Series #1 - AI/ML/GenAI\nTime: 8:30 AM - 12:00 PM\nLocation: AWS Vietnam Office, Bitexco Financial Tower\nDespite injury, attended event\nMorning Session:\n- Welcome \u0026amp; AI/ML landscape in Vietnam\n- AWS AI/ML Services Overview\n+ Amazon SageMaker platform\n+ Data preparation and labeling\n+ Model training, tuning, deployment\n+ MLOps capabilities\n+ Live Demo: SageMaker Studio\nAfter Coffee Break:\n- Generative AI with Amazon Bedrock\n+ Foundation Models (Claude, Llama, Titan)\n+ Prompt Engineering techniques\n+ RAG (Retrieval Augmented Generation)\n+ Bedrock Agents\n+ Guardrails\n+ Live Demo: GenAI chatbot with Bedrock 15/11/2025 15/11/2025 AWS Cloud Mastery #1 Week 10 Achievements Team Project Progress\nImplemented core project features Conducted successful team development sprint Integrated multiple components Performed code reviews and testing Maintained project documentation Coordinated effectively despite challenges AWS Cloud Mastery Series #1 - AI/ML/GenAI\nAttended comprehensive AI/ML workshop at AWS office Learned Amazon SageMaker end-to-end ML platform Understood ML model lifecycle (training, tuning, deployment) Explored MLOps capabilities Witnessed live SageMaker Studio demo Learned Generative AI with Amazon Bedrock Understood Foundation Models (Claude, Llama, Titan) Learned Prompt Engineering techniques Explored RAG architecture Understood Bedrock Agents and Guardrails Saw live GenAI chatbot demo Adaptability and Resilience\nManaged unexpected injury professionally Informed team promptly about situation Attended AWS event despite physical limitation Demonstrated commitment to learning Adapted workflow to accommodate injury Challenges Faced Hand injury on Friday significantly impacted ability to code Physical limitation affecting typing and hands-on work Balancing medical needs with project commitments Attending AWS event with injury required extra effort Coordinating with team while managing recovery Key Learnings Technical:\nSageMaker provides comprehensive ML platform MLOps is crucial for production ML systems Generative AI opens new possibilities with Foundation Models Prompt Engineering is key skill for GenAI applications RAG enhances LLM capabilities with custom knowledge Bedrock simplifies GenAI application development Personal:\nAdaptability is crucial when facing unexpected challenges Team support is invaluable during difficult times Commitment to learning transcends physical limitations Communication is key when circumstances change Health and safety should always be prioritized Next Week Goals Continue project work with adapted workflow Focus on tasks that accommodate physical limitation Attend doctor revisit appointment (21/11) Attend AWS Cloud Mastery Series #2 on DevOps (17/11) Maintain team coordination and communication Gradually resume normal activities as recovery permits "
},
{
	"uri": "/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives Attend AWS Cloud Mastery Series #2 on DevOps Continue team project with adapted workflow Attend first doctor revisit for injury follow-up Focus on tasks accommodating physical limitation Maintain learning momentum despite challenges Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (17/11) AWS Cloud Mastery Series #2 - DevOps (Full Day)\nTime: 8:30 AM - 5:00 PM\nLocation: AWS Vietnam Office, Bitexco Financial Tower\nMorning Session (8:30-12:00):\n- Welcome \u0026amp; DevOps Mindset (DORA metrics)\n- AWS DevOps Services - CI/CD Pipeline\n+ CodeCommit, CodeBuild, CodeDeploy, CodePipeline\n+ Git strategies, deployment patterns\n+ Demo: Full CI/CD pipeline\n- Infrastructure as Code\n+ CloudFormation templates\n+ AWS CDK\n+ Demo: IaC deployment\nAfternoon Session (13:00-17:00):\n- Container Services on AWS\n+ Docker fundamentals\n+ ECR, ECS, EKS\n+ App Runner\n+ Demo: Microservices deployment\n- Monitoring \u0026amp; Observability\n+ CloudWatch, X-Ray\n+ Demo: Full-stack observability\n- DevOps Best Practices \u0026amp; Case Studies\n- Q\u0026amp;A \u0026amp; Wrap-up 17/11/2025 17/11/2025 AWS Cloud Mastery #2 Tuesday (18/11) Project Work - Adapted Workflow\n- Review DevOps learnings from event\n- Team meeting (remote)\n- Focus on documentation tasks\n- Code review and planning activities\n- Coordinate with team on implementation 18/11/2025 18/11/2025 Team collaboration Wednesday (19/11) Project Coordination\n- Continue adapted project work\n- Documentation and design tasks\n- Team communication and planning\n- Review project progress\n- Identify tasks suitable for current capability 19/11/2025 19/11/2025 Project work Thursday (20/11) Project Planning \u0026amp; Documentation\n- Update project documentation\n- Architecture review and refinement\n- Planning for upcoming features\n- Team coordination meetings\n- Prepare for doctor visit 20/11/2025 20/11/2025 Documentation work Friday (21/11) Doctor Revisit #1\n- First follow-up appointment\n- Medical assessment of recovery progress\n- Received guidance on continued care\n- Discussed timeline for full recovery\n- Light project work after appointment 21/11/2025 21/11/2025 Medical follow-up Week 11 Achievements AWS Cloud Mastery Series #2 - DevOps\nAttended comprehensive full-day DevOps workshop Learned DevOps culture and DORA metrics Understood AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) Explored Git strategies and deployment patterns Witnessed full CI/CD pipeline demo Learned Infrastructure as Code with CloudFormation and CDK Saw IaC deployment demonstration Understood container services (Docker, ECR, ECS, EKS, App Runner) Witnessed microservices deployment demo Learned monitoring and observability with CloudWatch and X-Ray Saw full-stack observability demo Learned DevOps best practices and real-world case studies Adapted Project Workflow\nSuccessfully adapted work style to accommodate injury Focused on documentation and planning tasks Maintained team coordination and communication Contributed to project through code reviews Kept project momentum despite physical limitation Medical Follow-up\nAttended first doctor revisit appointment Received assessment of recovery progress Obtained guidance for continued care Understood timeline for full recovery Maintained positive outlook Continued Learning\nAbsorbed comprehensive DevOps knowledge Applied learnings to team project context Maintained engagement with AWS ecosystem Demonstrated commitment despite challenges Challenges Faced Physical limitation continued to affect hands-on coding Balancing full-day event attendance with recovery needs Adapting project contributions to current capabilities Managing fatigue from extended learning sessions Coordinating medical appointments with project commitments Key Learnings Technical - DevOps:\nCI/CD automation is crucial for modern development Infrastructure as Code enables reproducible deployments Container orchestration simplifies application management Observability is essential for production systems DevOps culture emphasizes collaboration and automation AWS provides comprehensive DevOps toolchain Personal:\nRecovery takes time and patience Adaptation enables continued productivity Documentation and planning are valuable contributions Team support facilitates continued participation Learning can continue despite physical limitations Medical care is essential for proper recovery Next Week Goals Continue adapted project work Prepare for AWS Cloud Mastery Series #3 on Security (29/11) Focus on project documentation and architecture Gradually increase hands-on involvement as recovery permits Maintain team coordination and communication Prepare for second doctor revisit (5/12) "
},
{
	"uri": "/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives Complete team project and prepare final presentation Attend AWS Cloud Mastery Series #3 on Security Reflect on 12-week AWS First Cloud Journey program Consolidate learnings and document achievements Plan for continued AWS learning journey Tasks Completed This Week Day Task Start Date Completion Date Reference Material Monday (24/11) Project Finalization\n- Team meeting for final sprint planning\n- Complete remaining project features\n- Code cleanup and optimization\n- Prepare project documentation\n- Plan final presentation structure 24/11/2025 24/11/2025 Project repository Tuesday (25/11) Project Testing \u0026amp; Documentation\n- Comprehensive testing of all features\n- Bug fixes and final adjustments\n- Complete technical documentation\n- Prepare user documentation\n- Create presentation slides 25/11/2025 25/11/2025 Project work Wednesday (26/11) Presentation Preparation\n- Finalize presentation content\n- Practice team presentation\n- Prepare demo scenarios\n- Review project highlights\n- Coordinate presentation roles 26/11/2025 26/11/2025 Presentation materials Thursday (27/11) Program Reflection \u0026amp; Consolidation\n- Review 12-week journey\n- Document key learnings\n- Consolidate AWS knowledge\n- Prepare for Security workshop\n- Update portfolio and resume 27/11/2025 27/11/2025 Personal reflection Friday (28/11) Final Project Polish\n- Last-minute improvements\n- Presentation rehearsal\n- Ensure all deliverables ready\n- Team coordination\n- Prepare for weekend event 28/11/2025 28/11/2025 Final preparations Saturday (29/11) AWS Cloud Mastery Series #3 - Security\nTime: 8:30 AM - 12:00 PM\nLocation: AWS Vietnam Office, Bitexco Financial Tower\nTopic: AWS Well-Architected Security Pillar\nAgenda:\n- Opening \u0026amp; Security Foundation\n+ Security Pillar in Well-Architected\n+ Least Privilege, Zero Trust, Defense in Depth\n+ Shared Responsibility Model\n- Pillar 1: Identity \u0026amp; Access Management\n+ IAM best practices\n+ IAM Identity Center\n+ MFA, credential rotation\n+ Demo: IAM Policy validation\n- Pillar 2: Detection\n+ CloudTrail, GuardDuty, Security Hub\n+ VPC Flow Logs\n+ EventBridge automation\n- Pillar 3: Infrastructure Protection\n+ VPC segmentation\n+ Security Groups vs NACLs\n+ WAF, Shield, Network Firewall\n- Pillar 4: Data Protection\n+ KMS encryption\n+ Secrets Manager\n+ Data classification\n- Pillar 5: Incident Response\n+ IR lifecycle\n+ Playbooks\n+ Auto-response with Lambda\n- Wrap-up \u0026amp; Q\u0026amp;A 29/11/2025 29/11/2025 AWS Cloud Mastery #3 Week 12 Achievements Team Project Completion\nSuccessfully completed team project Delivered all project features and functionality Created comprehensive documentation Prepared professional presentation Demonstrated effective team collaboration AWS Cloud Mastery Series #3 - Security\nAttended final Cloud Mastery workshop on Security Learned AWS Well-Architected Security Pillar Understood security principles (Least Privilege, Zero Trust, Defense in Depth) Learned Shared Responsibility Model Explored IAM best practices and Identity Center Understood security detection services (CloudTrail, GuardDuty, Security Hub) Learned infrastructure protection (VPC, Security Groups, WAF, Shield) Explored data protection with KMS and Secrets Manager Understood incident response lifecycle and automation Completed all three Cloud Mastery Series workshops 12-Week Program Reflection\nCompleted AWS First Cloud Journey program Gained comprehensive AWS knowledge across multiple domains Attended 6 major AWS events and workshops Translated 2+ AWS technical blogs Completed team project successfully Overcame challenges including injury and adapted workflow Built strong foundation for AWS career Key Skills Developed\nAWS core services (EC2, S3, VPC, RDS, Lambda) Serverless architecture and GenAI applications Data science and ML on AWS (SageMaker, Bedrock) DevOps practices and CI/CD pipelines Security best practices and Well-Architected Framework Team collaboration and project management Technical documentation and presentation skills Resilience and adaptability Challenges Faced Completing project while managing injury recovery Balancing final deliverables with event attendance Time pressure for project completion Coordinating team for final presentation Reflecting on extensive learning journey Key Learnings Technical:\nAWS provides comprehensive cloud platform Security must be considered at every layer Well-Architected Framework guides best practices Automation is key to scalable security Incident response requires preparation and practice Cloud skills are essential for modern technology careers Personal:\nConsistent learning leads to significant growth Challenges can be overcome with determination Team support is invaluable Hands-on practice reinforces theoretical knowledge Documentation preserves learning Continuous learning is essential in cloud computing Program Summary - 12 Weeks of Growth Weeks 1-4: AWS Fundamentals (EC2, S3, VPC, RDS)\nWeeks 5-7: Advanced Services (Lambda, Containers, Security)\nWeek 8: Midterm Exam\nWeeks 9-12: Team Project + Cloud Mastery Series (AI/ML, DevOps, Security)\nMajor Events Attended:\nAWS FCJ Kick-off (6/9) AWS Cloud Day 2025 (GenAI and Data track) Data Science on AWS Workshop (16/10) AWS Cloud Mastery #1 - AI/ML/GenAI (15/11) AWS Cloud Mastery #2 - DevOps (17/11) AWS Cloud Mastery #3 - Security (29/11) Achievements:\nCompleted comprehensive AWS learning program Passed midterm exam Translated AWS technical blogs Completed team project Attended 6 major AWS events Overcame injury and adapted workflow Built strong AWS foundation Next Steps Prepare for AWS certification exams Continue team project refinement Apply AWS skills in real-world projects Stay engaged with AWS community Explore advanced AWS services Pursue career opportunities in cloud computing "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]